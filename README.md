# DemandClean: A Multi-Objective Learning Framework for Balancing Model Tolerance to Data Authenticity and Diversity

This repository contains the implementation for our paper:  
**"DemandClean: A Multi-Objective Learning Framework for Balancing Model Tolerance to Data Authenticity and Diversity"**, which proposes a reinforcement learning-based framework to explore how machine learning models tolerate and respond to various types of data quality issues.

---

## ğŸ” Overview

DemandClean introduces a unified simulation environment that injects multiple types of data errors (missing values, outliers, and noise) and trains a **DQN-based RL agent** to make optimal cleaning decisions (`no-op`, `repair`, or `delete`) on a per-cell basis.

It balances two competing goals:
- **Authenticity**: preserving real-world data semantics by avoiding unnecessary modifications.
- **Diversity**: maintaining enough data variation and completeness for model generalization.

Our system evaluates and visualizes:
- Cleaning strategies under varying error rates;
- Model tolerance boundaries for different error types;
- Strategy shifts between repairing and deleting based on feature importance and error severity.

---

## ğŸ“ Project Structure
```
DemandClean/
â”œâ”€â”€ Data/                  # Datasets (adult, Bank, beers, Sick, etc.)
â”œâ”€â”€ saved_models/          # Trained DQN agents
â”‚   â””â”€â”€ default_agent.h5
â”œâ”€â”€ DQN_extract.py         # RL inference utilities
â”œâ”€â”€ experiments.py         # Main experiment pipeline
â”œâ”€â”€ WelcomeDemandClean.py  # ğŸŒ Streamlit-based front-end UI
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ README.md              # This file
```
---

## âš™ï¸ Installation

Install dependencies using:

```bash
pip install -r requirements.txt
```

If requirements.txt is not available, install manually:

pip install numpy pandas scikit-learn tensorflow gym tqdm matplotlib streamlit

ğŸ’¡ Recommended: Python 3.9+

---

ğŸš€ How to Run

1. Run the Core Experiment Pipeline
```bash
python experiments.py
```
By default, this runs the classification task using an SVM downstream model. You can customize:
```
task_type = 'classification'       # or 'regression'
n_episodes = 50                    # training episodes per error rate
error_rates = [0.1, 0.2, 0.3, ...] # error severity levels
model_type = 'svm'                 # downstream model: 'random_forest', 'logistic_regression', etc.
```
2. Launch the Interactive Streamlit UI ğŸŒ

To try an interactive front-end for exploring DemandClean:
```bash
streamlit run WelcomeDemandClean.py
```

---

ğŸ“Š Visualization & Results

The framework automatically generates and saves:
	â€¢	ğŸ“Š Bar charts comparing strategies (Do Nothing, Delete All, Repair All, DemandClean)
	â€¢	ğŸ“ˆ Line plots showing action trends vs. error rate
	â€¢	ğŸ“‰ Tolerance boundary analysis:
	â€¢	Overall model tolerance to errors
	â€¢	Strategy preference shift (repair vs. delete)
	â€¢	Error-type-specific sensitivity (missing vs. outlier)
	â€¢	ğŸ“ Final results saved in experiment_results.json

Sample figure outputs:
	â€¢	strategy_comparison*.png
	â€¢	tolerance_threshold*.png
	â€¢	repair_vs_delete_threshold*.png

---

ğŸ§  Core Features
	â€¢	âœ… Custom synthetic data generation (classification/regression)
	â€¢	âœ… Error injectors: missing values, outliers, and noise
	â€¢	âœ… DQN-based reinforcement learning cleaning agent
	â€¢	âœ… OpenAI Gym-style environment for modeling data cleaning as sequential decision making
	â€¢	âœ… Multiple cleaning strategy evaluations
	â€¢	âœ… Tolerance boundary estimation
	â€¢	âœ… ğŸ”§ Interactive Streamlit front-end for user-defined data exploration

---

ğŸ“œ Citation

If you find this project helpful, please consider citing:

@article{your2025demandclean,
  title={DemandClean: A Multi-Objective Learning Framework for Balancing Model Tolerance to Data Authenticity and Diversity},
  author={Zekai Qian and Xiaoou Ding and Hongzhi Wang},
  year={2025}
}
