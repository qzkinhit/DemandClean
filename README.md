# DemandClean: A Multi-Objective Learning Framework for Balancing Model Tolerance to Data Authenticity and Diversity

This repository contains the implementation for our paper:  
**"DemandClean: A Multi-Objective Learning Framework for Balancing Model Tolerance to Data Authenticity and Diversity"**, which proposes a reinforcement learning-based framework to explore how machine learning models tolerate and respond to various types of data quality issues.

---

## ğŸ” Overview

DemandClean introduces a unified simulation environment that injects multiple types of data errors (Missing Errors, Semantic Errors, and Syntactic Errors) and trains a **DQN-based RL agent** to make optimal cleaning decisions (`no-op`, `repair`, or `delete`) on a per-cell basis.

It balances two competing goals:
- **Authenticity**: preserving real-world data semantics by avoiding unnecessary modifications.
- **Diversity**: maintaining enough data variation and completeness for model generalization.

Our system evaluates and visualizes:
- Cleaning strategies under varying error rates;
- Model tolerance boundaries for different error types;
- Strategy shifts between repairing and deleting based on feature importance and error severity.

---

## ğŸ“ Project Structure
```
DemandClean/
â”œâ”€â”€ Data/                  # Datasets (adult, Bank, beers, Sick, etc.)
â”œâ”€â”€ saved_models/          # Trained DQN agents
â”‚   â””â”€â”€ default_agent.h5
â”œâ”€â”€ DQN_extract.py         # RL inference utilities
â”œâ”€â”€ experiments.py         # Main experiment pipeline
â”œâ”€â”€ WelcomeDemandClean.py  # ğŸŒ Streamlit-based front-end UI
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ README.md              # This file
```
---

## âš™ï¸ Installation

Install dependencies using:

```bash
pip install -r requirements.txt
```

If requirements.txt is not available, install manually:

pip install numpy pandas scikit-learn tensorflow gym tqdm matplotlib streamlit

ğŸ’¡ Recommended: Python 3.9+

---

ğŸš€ How to Run

1. Run the Core Experiment Pipeline
```bash
python experiments.py
```
By default, this runs the classification task using an SVM downstream model. You can customize:
```
task_type = 'classification'       # or 'regression'
n_episodes = 50                    # training episodes per error rate
error_rates = [0.1, 0.2, 0.3, ...] # error severity levels
model_type = 'svm'                 # downstream model: 'random_forest', 'logistic_regression', etc.
```
2. Launch the Interactive Streamlit UI ğŸŒ

To try an interactive front-end for exploring DemandClean:
```bash
streamlit run WelcomeDemandClean.py
```
---

## ğŸ“Š Visualization & Results

The framework automatically generates and saves:

- ğŸ“Š **Bar charts** comparing strategies:  
  *Do Nothing*, *Delete All*, *Repair All*, *DemandClean*

- ğŸ“ˆ **plots** showing action trends vs. error rate

- ğŸ“‰ **Tolerance boundary analysis**:
  - Overall model tolerance to errors  
  - Strategy preference shift (*repair* vs. *delete*)  
  - Error-type-specific sensitivity (*missing* vs. *outlier*)

- ğŸ“ Final results saved in `experiment_results.json`

---

## ğŸ§  Core Features

- âœ… **Custom synthetic data generation** (classification/regression)
- âœ… **Error injectors**: Missing Errors, Semantic Errors, and Syntactic Errors
- âœ… **DQN-based reinforcement learning cleaning agent**
- âœ… **OpenAI Gym-style environment** for modeling data cleaning as sequential decision making
- âœ… **Multiple cleaning strategy evaluations**
- âœ… **Tolerance boundary estimation**
- âœ… **Interactive Streamlit front-end** for user-defined data exploration

---

ğŸ“œ Citation

If you find this project helpful, please consider citing:
```
@article{demo4demandclean,
  title={DemandClean: A Multi-Objective Learning Framework for Balancing Model Tolerance to Data Authenticity and Diversity},
  author={Zekai Qian and Xiaoou Ding and Hongzhi Wang},
  year={2025}
}
```