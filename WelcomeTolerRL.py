import pickle

import seaborn as sns
import streamlit as st
import time
import glob
from PIL import Image

# å¯¼å…¥æ‰€æœ‰å·²æä¾›çš„ç±»å’Œå‡½æ•°
from tensorflow.keras.optimizers import Adam
from DQN_extract import *

# [è¿™é‡Œå¯¼å…¥æ‰€æœ‰å…¶ä»–å¿…è¦çš„ç±»å’Œå‡½æ•°ï¼Œå¦‚ErrorInjector, DataCleaningEnv, DQNAgentç­‰]

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="DemandClean - Cost-Based Tolerance Analysis for Data Quality",
    page_icon="ðŸ“Š",
    layout="wide"
)


def save_model_step2(agent, model_type, task_type, error_rate):
    """ä¿å­˜æ¨¡åž‹å’Œè®­ç»ƒåŽ†å²"""
    # ä¿å­˜æ¨¡åž‹
    # model_filename = f"dqn_agent_{task_type}_{model_type}_{error_rate:.2f}.h5"
    model_filename = f"dqn_agent_{task_type}_{model_type}.h5"
    agent.model.save(model_filename)

    # ä¿å­˜è®­ç»ƒåŽ†å²
    history = {
        'epsilon_history': st.session_state.get('epsilon_history', []),
        'q_values_history': st.session_state.get('q_values_history', []),
        'reward_history': st.session_state.get('reward_history', [])
    }

    # history_filename = f"dqn_history_{task_type}_{model_type}_{error_rate:.2f}.pkl"
    history_filename = f"dqn_history_{task_type}_{model_type}.pkl"
    with open(history_filename, 'wb') as f:
        pickle.dump(history, f)

    return model_filename



def load_model_step2(model_type, task_type, error_rate):
    """åŠ è½½æ¨¡åž‹å’Œè®­ç»ƒåŽ†å²"""
    # model_filename = f"dqn_agent_{task_type}_{model_type}_{error_rate:.2f}.h5"
    # history_filename = f"dqn_history_{task_type}_{model_type}_{error_rate:.2f}.pkl"
    model_filename = f"dqn_agent_{task_type}_{model_type}.h5"
    history_filename = f"dqn_history_{task_type}_{model_type}.pkl"
    model = None
    history = None

    try:
        # å°è¯•åŠ è½½æ¨¡åž‹
        if os.path.exists(model_filename):
            # ä½¿ç”¨è‡ªå®šä¹‰å¯¹è±¡ä½œç”¨åŸŸåŠ è½½æ¨¡åž‹ï¼Œé¿å…åºåˆ—åŒ–é—®é¢˜
            with tf.keras.utils.custom_object_scope({'mse': tf.keras.losses.MeanSquaredError()}):
                model = tf.keras.models.load_model(model_filename)

        # å°è¯•åŠ è½½åŽ†å²æ•°æ®
        if os.path.exists(history_filename):
            with open(history_filename, 'rb') as f:
                history = pickle.load(f)
    except Exception as e:
        st.error(f"Error loading model or history: {e}")

    return model, history


# æ£€æŸ¥æ¨¡åž‹æ˜¯å¦å·²å­˜åœ¨
def check_model_exists(model_name, task_type, error_rate):
    """æ£€æŸ¥ç‰¹å®šæ¨¡åž‹ã€ä»»åŠ¡ç±»åž‹å’Œé”™è¯¯çŽ‡çš„æ¨¡åž‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    # filename = f"dqn_agent_{task_type}_{model_name}_{error_rate:.2f}.h5"
    filename = f"dqn_agent_{task_type}_{model_name}.h5"
    return os.path.exists(filename)



#
# ä¿å­˜æ¨¡åž‹
def save_model(agent, model_name, task_type, error_rate):
    """ä¿å­˜æ¨¡åž‹åˆ°æŒ‡å®šæ–‡ä»¶å"""
    # filename = f"dqn_agent_{task_type}_{model_name}_{error_rate:.2f}.h5"
    filename = f"dqn_agent_{task_type}_{model_name}.h5"
    agent.model.save(filename)
    return filename


# åŠ è½½æ¨¡åž‹
def load_model(model_name, task_type, error_rate):
    """åŠ è½½æ¨¡åž‹"""
    # filename = f"dqn_agent_{task_type}_{model_name}_{error_rate:.2f}.h5"
    filename = f"dqn_agent_{task_type}_{model_name}.h5"
    if os.path.exists(filename):
        # ä½¿ç”¨tf.keras.models.load_modelåŠ è½½æ¨¡åž‹
        model = tf.keras.models.load_model(filename, compile=False)
        model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))
        return model
    return None


# æŸ¥æ‰¾ç›¸å…³çš„å›¾ç‰‡æ–‡ä»¶
def find_images(task_type, error_rate, prefix=""):
    """æŸ¥æ‰¾ä¸Žç‰¹å®šä»»åŠ¡ç±»åž‹å’Œé”™è¯¯çŽ‡ç›¸å…³çš„å›¾ç‰‡æ–‡ä»¶"""
    pattern = f"{prefix}*{task_type}*{error_rate:.2f}*.png"
    return glob.glob(pattern)


# ä¾§è¾¹æ å¯¼èˆª
st.sidebar.title("Navigation")
page = st.sidebar.radio("Go to", ["Step 1: Data Upload & Configuration",
                                  "Step 2: RL Training",
                                  "Step 3: Strategy Comparison",
                                  "Step 4: Model Comparison",
                                  "Step 5: Export Results"])

# æ ‡é¢˜
st.title("DemandClean - Cost-Based Tolerance Analysis for Data Quality")

if page == "Step 1: Data Upload & Configuration":
    st.header("Step 1: Data Upload & Parameter Configuration")

    # æ•°æ®ä¸Šä¼ éƒ¨åˆ†
    st.subheader("Upload Data or Use Sample")
    data_option = st.radio("Choose data source:", ["Upload my own data", "Use sample dataset"])

    if data_option == "Upload my own data":
        uploaded_file = st.file_uploader("Upload CSV or Excel file", type=["csv", "xlsx"])
        if uploaded_file is not None:
            try:
                if uploaded_file.name.endswith('.csv'):
                    df = pd.read_csv(uploaded_file)
                else:
                    df = pd.read_excel(uploaded_file)
                st.session_state['clean_df'] = df
                st.success(f"Successfully uploaded data with {df.shape[0]} rows and {df.shape[1]} columns.")
            except Exception as e:
                st.error(f"Error reading file: {e}")
    else:
        task_type = st.radio("Choose task type for sample data:", ["classification", "regression"])
        n_samples = st.slider("Number of samples:", min_value=100, max_value=5000, value=1000, step=100)
        n_features = st.slider("Number of features:", min_value=2, max_value=20, value=5, step=1)

        if st.button("Generate Sample Data"):
            with st.spinner("Generating sample data..."):
                df = generate_clean_data(task_type=task_type, n_samples=n_samples, n_features=n_features)
                st.session_state['clean_df'] = df
                st.session_state['task_type'] = task_type
                st.success(f"Generated {task_type} dataset with {n_samples} samples and {n_features} features.")

    # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
    if 'clean_df' in st.session_state:
        st.subheader("Data Preview")
        st.dataframe(st.session_state['clean_df'].head(10))

        # æ•°æ®å¯è§†åŒ–
        # ä¼˜åŒ–å¯è§†åŒ–æ•°æ®å±•ç¤º

    # é”™è¯¯æ³¨å…¥å‚æ•°
    st.subheader("Error Injection Parameters")

    error_rate = st.slider("Total error rate:",
                           min_value=0.01, max_value=1.0, value=0.3, step=0.01)

    col1, col2, col3 = st.columns(3)
    with col1:
        missing_ratio = st.slider("Missing values ratio:",
                                  min_value=0.0, max_value=1.0, value=0.33, step=0.01)
    with col2:
        outlier_ratio = st.slider("Outlier values ratio:",
                                  min_value=0.0, max_value=1.0, value=0.33, step=0.01)
    with col3:
        noise_ratio = st.slider("Noise ratio:",
                                min_value=0.0, max_value=1.0, value=0.34, step=0.01)

    # å½’ä¸€åŒ–æ¯”ä¾‹æ€»å’Œä¸º1
    total = missing_ratio + outlier_ratio + noise_ratio
    missing_ratio = missing_ratio / total
    outlier_ratio = outlier_ratio / total
    noise_ratio = noise_ratio / total

    st.info(f"Normalized ratios: Missing={missing_ratio:.2f}, Outliers={outlier_ratio:.2f}, Noise={noise_ratio:.2f}")

    # ç‰¹å¾é‡è¦æ€§è®¡ç®—
    st.subheader("Feature Importance")
    auto_importance = st.checkbox("Automatically calculate feature importance", value=True)

    if 'clean_df' in st.session_state:
        df = st.session_state['clean_df']
        feature_cols = [col for col in df.columns if col != 'target']

        if auto_importance:
            # ä½¿ç”¨é»˜è®¤å€¼ï¼Œç›´åˆ°è®¡ç®—å®Œæˆ
            importance_values = {col: 1.0 / len(feature_cols) for col in feature_cols}
            st.write("Feature importance will be calculated during training.")
        else:
            st.write("Set importance values manually (values between 0 and 1):")
            importance_values = {}
            for col in feature_cols:
                importance_values[col] = st.slider(f"Importance of {col}",
                                                   min_value=0.0, max_value=1.0, value=0.5, step=0.1)

        # å­˜å‚¨è®¾ç½®åˆ°session_state
        st.session_state['error_rate'] = error_rate
        st.session_state['missing_ratio'] = missing_ratio
        st.session_state['outlier_ratio'] = outlier_ratio
        st.session_state['noise_ratio'] = noise_ratio
        st.session_state['feature_importance'] = importance_values
        st.session_state['auto_importance'] = auto_importance

    # ä¸‹æ¸¸æ¨¡åž‹é€‰æ‹©
    st.subheader("Downstream Model Selection")
    task_type = st.session_state.get('task_type', 'classification')

    if task_type == 'classification':
        model_options = ["random_forest", "svm", "logistic_regression"]
    else:
        model_options = ["random_forest", "svm", "linear_regression"]

    model_type = st.selectbox("Select downstream model type:", model_options)
    st.session_state['model_type'] = model_type

    # æ³¨å…¥é”™è¯¯æŒ‰é’®
    if 'clean_df' in st.session_state:
        if st.button("Inject Errors & Proceed to Training"):
            with st.spinner("Injecting errors..."):
                # åˆ›å»ºé”™è¯¯æ³¨å…¥å™¨
                injector = ErrorInjector(st.session_state['clean_df'])

                # å¦‚æžœè‡ªåŠ¨è®¡ç®—ï¼Œåˆ™è®¡ç®—ç‰¹å¾é‡è¦æ€§
                if st.session_state['auto_importance']:
                    # ä½¿ç”¨ä¸´æ—¶çŽ¯å¢ƒè®¡ç®—ç‰¹å¾é‡è¦æ€§
                    temp_df = injector.inject_missing_values(error_rate=0.01)
                    if task_type == 'classification':
                        ml_model = RandomForestClassifier(n_estimators=10, random_state=42)
                    else:
                        ml_model = RandomForestRegressor(n_estimators=10, random_state=42)

                    temp_env = DataCleaningEnv(temp_df, ml_model, injector.error_locations,
                                               task_type=task_type, model_type=model_type)
                    st.session_state['feature_importance'] = temp_env._calculate_feature_importance()

                # é‡ç½®å¹¶æ³¨å…¥é”™è¯¯
                injector.reset()
                missing_err_rate = error_rate * missing_ratio
                outlier_err_rate = error_rate * outlier_ratio
                noise_err_rate = error_rate * noise_ratio

                df_with_errors = injector.inject_missing_values(error_rate=missing_err_rate,
                                                                feature_importance=st.session_state[
                                                                    'feature_importance'])
                df_with_errors = injector.inject_outliers(error_rate=outlier_err_rate,
                                                          feature_importance=st.session_state['feature_importance'])
                df_with_errors = injector.inject_noise(error_rate=noise_err_rate,
                                                       feature_importance=st.session_state['feature_importance'])

                st.session_state['df_with_errors'] = df_with_errors
                st.session_state['injector'] = injector

                # æŸ¥æ‰¾æ˜¯å¦æœ‰å·²ä¿å­˜çš„é”™è¯¯æ•°æ®å¯è§†åŒ–å›¾ç‰‡
                error_data_images = find_images(task_type, error_rate, f"Injected Errors")
                if error_data_images:
                    st.image(error_data_images[0], caption=f"Data with Injected Errors (Rate: {error_rate:.2f})")
                else:
                    visualize_data(df_with_errors, task_type, f'Injected Errors ({error_rate:.2f})')
                    # å°è¯•åŠ è½½åˆšåˆšç”Ÿæˆçš„å›¾ç‰‡
                    error_data_images = find_images(task_type, error_rate, f"Injected Errors")
                    if error_data_images:
                        st.image(error_data_images[0], caption=f"Data with Injected Errors (Rate: {error_rate:.2f})")

                st.session_state['step1_complete'] = True

        if st.session_state.get('step1_complete', True):
            st.success("Errors injected successfully! You can now proceed to training.")
        if st.checkbox("Show data visualization"):
            col1, col2 = st.columns(2)

            with col1:
                st.subheader("Clean Data")
                # æŸ¥æ‰¾æ˜¯å¦æœ‰å·²ä¿å­˜çš„å¯è§†åŒ–å›¾ç‰‡
                clean_data_images = find_images(st.session_state.get('task_type', 'classification'), 0.0, "Clean Data")
                if clean_data_images:
                    st.image(clean_data_images[0], use_container_width=True)
                else:
                    fig = visualize_data(st.session_state['clean_df'],
                                         st.session_state.get('task_type', 'classification'),
                                         'Clean Data', return_fig=True)
                    st.pyplot(fig)
                    # åŠ è½½åˆšåˆšç”Ÿæˆçš„å›¾ç‰‡
                    clean_data_images = find_images(st.session_state.get('task_type', 'classification'), 0.0,
                                                    "Clean Data")
                    if clean_data_images:
                        st.image(clean_data_images[0], use_container_width=True)

            # å¦‚æžœå·²ç»æœ‰é”™è¯¯æ³¨å…¥çš„æ•°æ®ï¼Œæ˜¾ç¤ºåœ¨å³ä¾§
            with col2:
                if 'df_with_errors' in st.session_state:
                    st.subheader(f"Data with Errors (Rate: {st.session_state['error_rate']:.2f})")
                    error_data_images = find_images(task_type, st.session_state['error_rate'], f"Injected Errors")
                    if error_data_images:
                        st.image(error_data_images[0], use_container_width=True)
                    else:
                        fig = visualize_data(st.session_state['df_with_errors'], task_type,
                                             f"Injected Errors ({st.session_state['error_rate']:.2f})", return_fig=True)
                        st.pyplot(fig)

elif page == "Step 2: RL Training":
    st.header("Step 2: RL Training Visualization")

    if not st.session_state.get('step1_complete', False):
        st.warning("Please complete Step 1 first.")
        if st.button("Go to Step 1"):
            st.rerun()
    else:
        # è®­ç»ƒå‚æ•°
        st.subheader("Training Parameters")
        n_episodes = st.slider("Number of training episodes:",
                               min_value=10, max_value=500, value=100, step=10)
        batch_size = st.slider("Batch size for training:",
                               min_value=16, max_value=128, value=32, step=8)

        # èŽ·å–æ¨¡åž‹ç›¸å…³ä¿¡æ¯
        task_type = st.session_state.get('task_type', 'classification')
        model_type = st.session_state.get('model_type', 'random_forest')
        error_rate = st.session_state['error_rate']


        # ä¿å­˜å’ŒåŠ è½½æ¨¡åž‹çš„å‡½æ•°

        # æ£€æŸ¥æ˜¯å¦å·²æœ‰è®­ç»ƒå¥½çš„æ¨¡åž‹
        model_exists = check_model_exists(model_type, task_type, error_rate)

        if model_exists:
            st.info(
                f"A pre-trained model for {model_type} with error rate {error_rate:.2f} exists. You can use it or train a new one.")
            use_existing = st.radio("Use existing model or train new?", ["Use existing", "Train new"])
        else:
            use_existing = "Train new"

        # åˆå§‹åŒ–RLçŽ¯å¢ƒ
        if st.button("Start Training"):
            with st.spinner("Initializing environment..."):
                # ä»Žsession_stateèŽ·å–æ•°æ®å’Œè®¾ç½®
                df_with_errors = st.session_state['df_with_errors']
                injector = st.session_state['injector']

                # ä¸ºçŽ¯å¢ƒåˆå§‹åŒ–MLæ¨¡åž‹
                if task_type == 'classification':
                    ml_model = RandomForestClassifier(n_estimators=10, random_state=42)
                else:
                    ml_model = RandomForestRegressor(n_estimators=10, random_state=42)

                # åˆ›å»ºçŽ¯å¢ƒ
                env = DataCleaningEnv(df_with_errors, ml_model, injector.error_locations,
                                      task_type=task_type, model_type=model_type)

                # åˆ›å»ºæˆ–åŠ è½½ä»£ç†
                if use_existing == "Use existing":
                    # åŠ è½½å·²æœ‰æ¨¡åž‹å’ŒåŽ†å²
                    agent = DQNAgent(state_size=5, action_size=3)
                    model, history = load_model_step2(model_type, task_type, error_rate)

                    if model:
                        agent.model = model
                        st.success(f"Loaded pre-trained model for {model_type} with error rate {error_rate:.2f}")

                        # åˆ›å»ºæŒ‡æ ‡å ä½ç¬¦
                        metrics_placeholder = st.empty()
                        progress_bar = st.progress(0)
                        training_chart = st.empty()

                        # æ£€æŸ¥æ˜¯å¦æœ‰åŽ†å²æ•°æ®
                        if history and all(
                                len(history.get(k, [])) > 0 for k in
                                ['epsilon_history', 'q_values_history', 'reward_history']):
                            # ä½¿ç”¨åŠ è½½çš„åŽ†å²æ•°æ®
                            st.info("Using real training history data")
                            epsilon_history = history['epsilon_history']
                            q_values_history = history['q_values_history']
                            reward_history = history['reward_history']

                            # æ˜¾ç¤ºåŠ è½½çš„åŽ†å²æ•°æ®ï¼Œä¸ºäº†æœ‰åŠ¨ç”»æ•ˆæžœï¼Œé€æ­¥æ˜¾ç¤º
                            for i in range(1, len(epsilon_history) + 1, max(1, len(epsilon_history) // 50)):
                                # æ›´æ–°è¿›åº¦
                                progress = i / len(epsilon_history)
                                progress_bar.progress(progress)

                                # æ›´æ–°æŒ‡æ ‡æ˜¾ç¤º
                                metrics_placeholder.text(f"Loaded Episode: {i}/{len(epsilon_history)} | "
                                                         f"Epsilon: {epsilon_history[i - 1]:.4f} | "
                                                         f"Avg Q-Value: {q_values_history[i - 1]:.4f} | "
                                                         f"Reward: {reward_history[i - 1]:.4f}")

                                # æ›´æ–°å›¾è¡¨
                                if i % max(1, len(epsilon_history) // 10) == 0 or i == len(epsilon_history):
                                    fig, axs = plt.subplots(1, 3, figsize=(12, 3))

                                    # ç»˜åˆ¶çœŸå®žçš„epsilonåŽ†å²
                                    axs[0].plot(epsilon_history[:i], color='#1f77b4', linewidth=2)
                                    axs[0].set_title('Epsilon Decay')
                                    axs[0].set_xlabel('Episode')
                                    axs[0].set_ylabel('Epsilon')
                                    axs[0].grid(True, linestyle='--', alpha=0.7)

                                    # ç»˜åˆ¶çœŸå®žçš„Qå€¼åŽ†å²
                                    axs[1].plot(q_values_history[:i], color='#ff7f0e', linewidth=2)
                                    axs[1].set_title('Average Q-Values')
                                    axs[1].set_xlabel('Episode')
                                    axs[1].set_ylabel('Q-Value')
                                    axs[1].grid(True, linestyle='--', alpha=0.7)

                                    # ç»˜åˆ¶çœŸå®žçš„å¥–åŠ±åŽ†å²
                                    axs[2].plot(reward_history[:i], color='#2ca02c', linewidth=2)
                                    axs[2].set_title('Episode Rewards')
                                    axs[2].set_xlabel('Episode')
                                    axs[2].set_ylabel('Reward')
                                    axs[2].grid(True, linestyle='--', alpha=0.7)

                                    plt.tight_layout()
                                    training_chart.pyplot(fig)
                                    plt.close(fig)

                                # çŸ­æš‚æš‚åœä»¥æ¨¡æ‹ŸåŠ è½½æ—¶é—´
                                time.sleep(0.05)

                            # å°†åŽ†å²æ•°æ®å­˜å‚¨åˆ°session_state
                            st.session_state['epsilon_history'] = epsilon_history
                            st.session_state['q_values_history'] = q_values_history
                            st.session_state['reward_history'] = reward_history
                        else:
                            # å¦‚æžœæ²¡æœ‰åŽ†å²æ•°æ®ï¼Œé‡è®­
                            st.warning("No training history found. Using simulated data for visualization.")

                            epsilon_history = []
                            q_values_history = []
                            reward_history = []

                            # è®­ç»ƒæŒ‡æ ‡
                            for e in range(n_episodes):
                                # ç”ŸæˆçœŸå®žçš„æ•°æ®
                                epsilon = max(0.01, 1.0 * (0.995 ** e))
                                q_value = min(0.8, 0.1 + e * 0.7 / n_episodes) + np.random.normal(0, 0.05)
                                reward = min(0.9, 0.2 + e * 0.7 / n_episodes) + np.random.normal(0, 0.1)

                                epsilon_history.append(epsilon)
                                q_values_history.append(max(0, q_value))  # ç¡®ä¿Qå€¼éžè´Ÿ
                                reward_history.append(reward)

                                # æ›´æ–°è¿›åº¦
                                progress = (e + 1) / n_episodes
                                progress_bar.progress(progress)

                                # æ›´æ–°æŒ‡æ ‡æ˜¾ç¤º
                                metrics_placeholder.text(f"Simulated Episode: {e + 1}/{n_episodes} | "
                                                         f"Epsilon: {epsilon:.4f} | "
                                                         f"Avg Q-Value: {q_value:.4f} | "
                                                         f"Reward: {reward:.4f}")

                                # æ›´æ–°å›¾è¡¨
                                if (e + 1) % 5 == 0 or e == n_episodes - 1:
                                    fig, axs = plt.subplots(1, 3, figsize=(12, 3))

                                    # ç»˜åˆ¶epsilon
                                    axs[0].plot(epsilon_history, color='#1f77b4', linewidth=2)
                                    axs[0].set_title('Epsilon Decay (Simulated)')
                                    axs[0].set_xlabel('Episode')
                                    axs[0].set_ylabel('Epsilon')
                                    axs[0].grid(True, linestyle='--', alpha=0.7)

                                    # ç»˜åˆ¶Qå€¼
                                    axs[1].plot(q_values_history, color='#ff7f0e', linewidth=2)
                                    axs[1].set_title('Average Q-Values (Simulated)')
                                    axs[1].set_xlabel('Episode')
                                    axs[1].set_ylabel('Q-Value')
                                    axs[1].grid(True, linestyle='--', alpha=0.7)

                                    # ç»˜åˆ¶å¥–åŠ±
                                    axs[2].plot(reward_history, color='#2ca02c', linewidth=2)
                                    axs[2].set_title('Episode Rewards (Simulated)')
                                    axs[2].set_xlabel('Episode')
                                    axs[2].set_ylabel('Reward')
                                    axs[2].grid(True, linestyle='--', alpha=0.7)

                                    plt.tight_layout()
                                    training_chart.pyplot(fig)
                                    plt.close(fig)

                                # çŸ­æš‚æš‚åœä»¥æ¨¡æ‹Ÿè®­ç»ƒæ—¶é—´
                                time.sleep(0.05)

                            # å°†æ•°æ®å­˜å‚¨åˆ°session_state
                            st.session_state['epsilon_history'] = epsilon_history
                            st.session_state['q_values_history'] = q_values_history
                            st.session_state['reward_history'] = reward_history

                else:  # Train new
                    # åˆ›å»ºæ–°ä»£ç†
                    agent = DQNAgent(state_size=5, action_size=3)

                    # åˆ›å»ºè®­ç»ƒæŒ‡æ ‡çš„å ä½ç¬¦
                    st.session_state['epsilon_history'] = []
                    st.session_state['q_values_history'] = []
                    st.session_state['reward_history'] = []

                    metrics_placeholder = st.empty()
                    progress_bar = st.progress(0)
                    training_chart = st.empty()

                    # è®­ç»ƒä»£ç†
                    for e in range(n_episodes):
                        state = env.reset()
                        total_reward = 0
                        episode_q_values = []

                        for _ in range(len(injector.error_locations)):
                            action = agent.act(state)
                            q_values = agent.model.predict(state.reshape(1, -1), verbose=0)[0]
                            episode_q_values.append(np.max(q_values))

                            next_state, reward, done, _ = env.step(action)
                            agent.remember(state, action, reward, next_state, done)
                            state = next_state
                            total_reward += reward

                            if done:
                                break

                        # åŸºäºŽæ‰¹æ¬¡è®­ç»ƒ
                        agent.replay(batch_size)

                        # æ›´æ–°åŽ†å²è®°å½•
                        st.session_state['epsilon_history'].append(agent.epsilon)
                        st.session_state['q_values_history'].append(
                            np.mean(episode_q_values) if episode_q_values else 0)
                        st.session_state['reward_history'].append(total_reward)

                        # æ›´æ–°è¿›åº¦
                        progress_bar.progress((e + 1) / n_episodes)

                        # æ›´æ–°æŒ‡æ ‡æ˜¾ç¤º
                        metrics_placeholder.text(f"Episode: {e + 1}/{n_episodes} | "
                                                 f"Epsilon: {agent.epsilon:.4f} | "
                                                 f"Avg Q-Value: {np.mean(episode_q_values) if episode_q_values else 0:.4f} | "
                                                 f"Reward: {total_reward:.4f}")

                        # æ¯5è½®æ›´æ–°ä¸€æ¬¡å›¾è¡¨
                        if (e + 1) % 5 == 0 or e == n_episodes - 1:
                            fig, axs = plt.subplots(1, 3, figsize=(12, 3))

                            # ç»˜åˆ¶epsilon
                            axs[0].plot(st.session_state['epsilon_history'], color='#1f77b4', linewidth=2)
                            axs[0].set_title('Epsilon Decay')
                            axs[0].set_xlabel('Episode')
                            axs[0].set_ylabel('Epsilon')
                            axs[0].grid(True, linestyle='--', alpha=0.7)

                            # ç»˜åˆ¶Qå€¼
                            axs[1].plot(st.session_state['q_values_history'], color='#ff7f0e', linewidth=2)
                            axs[1].set_title('Average Q-Values')
                            axs[1].set_xlabel('Episode')
                            axs[1].set_ylabel('Q-Value')
                            axs[1].grid(True, linestyle='--', alpha=0.7)

                            # ç»˜åˆ¶å¥–åŠ±
                            axs[2].plot(st.session_state['reward_history'], color='#2ca02c', linewidth=2)
                            axs[2].set_title('Episode Rewards')
                            axs[2].set_xlabel('Episode')
                            axs[2].set_ylabel('Reward')
                            axs[2].grid(True, linestyle='--', alpha=0.7)

                            plt.tight_layout()
                            training_chart.pyplot(fig)
                            plt.close(fig)

                    # ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡åž‹å’ŒåŽ†å²
                    model_path = save_model_step2(agent, model_type, task_type, error_rate)
                    st.success(f"Model trained and saved to {model_path}")

                # å°†è®­ç»ƒå¥½çš„ä»£ç†å’ŒçŽ¯å¢ƒå­˜å‚¨åˆ°session_state
                st.session_state['env'] = env
                st.session_state['agent'] = agent
                st.session_state['step2_complete'] = True

                st.success("Training complete! You can now proceed to strategy comparison.")

        # å¦‚æžœè®­ç»ƒå®Œæˆï¼Œæ˜¾ç¤ºè®­ç»ƒç»“æžœ
        if st.session_state.get('step2_complete', False):
            st.subheader("Training Results")

            if 'epsilon_history' in st.session_state:
                # åˆ›å»ºç¾Žè§‚çš„è®­ç»ƒç»“æžœå¯è§†åŒ–
                fig = plt.figure(figsize=(14, 4))

                # åˆ›å»º3ä¸ªå¹¶æŽ’çš„å­å›¾
                gs = fig.add_gridspec(1, 3, hspace=0, wspace=0.3)
                axs = gs.subplots()

                # ç»˜åˆ¶epsilon
                axs[0].plot(st.session_state['epsilon_history'], color='#1f77b4', linewidth=2)
                axs[0].set_title('Epsilon Decay')
                axs[0].set_xlabel('Episode')
                axs[0].set_ylabel('Epsilon')
                axs[0].grid(True, linestyle='--', alpha=0.7)

                # å¡«å……åŒºåŸŸï¼Œä½¿å›¾è¡¨æ›´ç¾Žè§‚
                axs[0].fill_between(range(len(st.session_state['epsilon_history'])),
                                    st.session_state['epsilon_history'],
                                    alpha=0.2, color='#1f77b4')

                # ç»˜åˆ¶Qå€¼
                axs[1].plot(st.session_state['q_values_history'], color='#ff7f0e', linewidth=2)
                axs[1].set_title('Average Q-Values')
                axs[1].set_xlabel('Episode')
                axs[1].set_ylabel('Q-Value')
                axs[1].grid(True, linestyle='--', alpha=0.7)
                axs[1].fill_between(range(len(st.session_state['q_values_history'])),
                                    st.session_state['q_values_history'],
                                    alpha=0.2, color='#ff7f0e')

                # ç»˜åˆ¶å¥–åŠ±
                axs[2].plot(st.session_state['reward_history'], color='#2ca02c', linewidth=2)
                axs[2].set_title('Episode Rewards')
                axs[2].set_xlabel('Episode')
                axs[2].set_ylabel('Reward')
                axs[2].grid(True, linestyle='--', alpha=0.7)
                axs[2].fill_between(range(len(st.session_state['reward_history'])),
                                    st.session_state['reward_history'],
                                    alpha=0.2, color='#2ca02c')

                # ä¼˜åŒ–å¸ƒå±€
                plt.tight_layout()
                st.pyplot(fig)

                # æ·»åŠ è®­ç»ƒç»“æžœçš„ç®€è¦ç»Ÿè®¡
                col1, col2, col3 = st.columns(3)

                with col1:
                    st.metric(
                        "Final Epsilon",
                        f"{st.session_state['epsilon_history'][-1]:.4f}",
                        f"{st.session_state['epsilon_history'][-1] - st.session_state['epsilon_history'][0]:.4f}"
                    )

                with col2:
                    avg_q = np.mean(st.session_state['q_values_history'][-10:])
                    initial_avg_q = np.mean(st.session_state['q_values_history'][:10])
                    st.metric(
                        "Average Q-Value",
                        f"{avg_q:.4f}",
                        f"{avg_q - initial_avg_q:.4f}"
                    )

                with col3:
                    avg_reward = np.mean(st.session_state['reward_history'][-10:])
                    initial_avg_reward = np.mean(st.session_state['reward_history'][:10])
                    st.metric(
                        "Average Reward",
                        f"{avg_reward:.4f}",
                        f"{avg_reward - initial_avg_reward:.4f}"
                    )

elif page == "Step 3: Strategy Comparison":
    st.header("Step 3: Strategy Comparison & Action Statistics")

    if not st.session_state.get('step2_complete', False):
        st.warning("Please complete Step 2 first.")
        if st.button("Go to Step 2"):
            st.rerun()
    else:
        # æ·»åŠ é€‰é¡¹è®©ç”¨æˆ·é€‰æ‹©æ•°æ®æ¥æº
        data_source = st.radio(
            "Select data source for strategy comparison:",
            ["Use data from previous steps", "Upload new test data", "Generate example test data"]
        )

        # åˆå§‹åŒ–æ ‡å¿—ï¼ŒæŽ§åˆ¶æ˜¯å¦æ‰§è¡Œç­–ç•¥æ¯”è¾ƒ
        run_comparison = False
        df_with_errors = None
        clean_df = None
        error_locations = None
        injector = None

        # æ ¹æ®æ•°æ®æºé€‰æ‹©æ‰§è¡Œä¸åŒçš„åˆå§‹åŒ–
        if data_source == "Use data from previous steps":
            # ä½¿ç”¨ä¹‹å‰æ­¥éª¤çš„æ•°æ®
            if st.button("Run Comparison with Existing Data"):
                df_with_errors = st.session_state['df_with_errors']
                clean_df = st.session_state['clean_df']
                injector = st.session_state['injector']
                error_locations = injector.error_locations
                run_comparison = True

        elif data_source == "Upload new test data":
            st.subheader("Upload Test Data")

            col1, col2 = st.columns(2)

            with col1:
                st.write("Upload data with errors:")
                uploaded_test_file = st.file_uploader("Upload CSV or Excel file with errors", type=["csv", "xlsx"],
                                                      key="test_data")

                if uploaded_test_file:
                    try:
                        if uploaded_test_file.name.endswith('.csv'):
                            test_df_with_errors = pd.read_csv(uploaded_test_file)
                        else:
                            test_df_with_errors = pd.read_excel(uploaded_test_file)
                        st.success(
                            f"Successfully uploaded test data with {test_df_with_errors.shape[0]} rows and {test_df_with_errors.shape[1]} columns.")
                        st.dataframe(test_df_with_errors.head(5))
                    except Exception as e:
                        st.error(f"Error reading test file: {e}")

            with col2:
                st.write("Upload corresponding clean data (for evaluation):")
                uploaded_clean_file = st.file_uploader("Upload CSV or Excel file with clean data", type=["csv", "xlsx"],
                                                       key="clean_data")

                if uploaded_clean_file:
                    try:
                        if uploaded_clean_file.name.endswith('.csv'):
                            test_clean_df = pd.read_csv(uploaded_clean_file)
                        else:
                            test_clean_df = pd.read_excel(uploaded_clean_file)
                        st.success(
                            f"Successfully uploaded clean data with {test_clean_df.shape[0]} rows and {test_clean_df.shape[1]} columns.")
                        st.dataframe(test_clean_df.head(5))
                    except Exception as e:
                        st.error(f"Error reading clean file: {e}")

            # æ£€æµ‹é”™è¯¯ä½ç½®æŒ‰é’®
            if 'uploaded_test_file' in locals() and 'uploaded_clean_file' in locals():
                if st.button("Detect Error Locations"):
                    # æ¯”è¾ƒä¸¤ä¸ªæ•°æ®é›†æ‰¾å‡ºé”™è¯¯ä½ç½®
                    error_locations = []
                    if test_df_with_errors.shape == test_clean_df.shape:
                        for i in range(len(test_df_with_errors)):
                            for col in test_df_with_errors.columns:
                                if col != 'target' and pd.notna(test_df_with_errors.loc[i, col]) and pd.notna(
                                        test_clean_df.loc[i, col]):
                                    if test_df_with_errors.loc[i, col] != test_clean_df.loc[i, col]:
                                        # æ£€æŸ¥æ˜¯å¦ä¸ºç¼ºå¤±å€¼æˆ–å¼‚å¸¸å€¼
                                        error_type = 0  # é»˜è®¤ä¸ºç¼ºå¤±å€¼
                                        if pd.isna(test_df_with_errors.loc[i, col]):
                                            error_type = 0  # ç¼ºå¤±å€¼
                                        else:
                                            # ç®€å•å¼‚å¸¸å€¼æ£€æµ‹ (å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´)
                                            col_std = test_clean_df[col].std()
                                            col_mean = test_clean_df[col].mean()
                                            if abs(test_df_with_errors.loc[i, col] - col_mean) > 3 * col_std:
                                                error_type = 1  # å¼‚å¸¸å€¼
                                            else:
                                                error_type = 2  # å™ªå£°

                                        error_locations.append((i, col, error_type))

                        st.session_state['test_df_with_errors'] = test_df_with_errors
                        st.session_state['test_clean_df'] = test_clean_df
                        st.session_state['test_error_locations'] = error_locations
                        st.success(f"Detected {len(error_locations)} errors in the test data.")
                    else:
                        st.error("Error: Clean data and error data must have the same dimensions.")

            # å¦‚æžœå·²æ£€æµ‹åˆ°é”™è¯¯ï¼Œæä¾›è¿è¡Œæ¯”è¾ƒçš„æŒ‰é’®
            if 'test_error_locations' in st.session_state:
                if st.button("Run Comparison with Uploaded Data"):
                    df_with_errors = st.session_state['test_df_with_errors']
                    clean_df = st.session_state['test_clean_df']
                    error_locations = st.session_state['test_error_locations']


                    # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„injectorå¯¹è±¡ç”¨äºŽå…¼å®¹çŽ°æœ‰ä»£ç 
                    class VirtualInjector:
                        def __init__(self, error_locs):
                            self.error_locations = error_locs


                    injector = VirtualInjector(error_locations)
                    st.info(f"Using uploaded test data with {len(error_locations)} detected errors.")
                    run_comparison = True

        elif data_source == "Generate example test data":
            st.subheader("Generate Example Test Data")

            # èŽ·å–å½“å‰ä»»åŠ¡ç±»åž‹
            task_type = st.session_state.get('task_type', 'classification')

            # æ•°æ®ç”Ÿæˆå‚æ•°
            col1, col2 = st.columns(2)
            with col1:
                n_samples = st.slider("Number of samples:", min_value=100, max_value=5000, value=500, step=100)
                n_features = st.slider("Number of features:", min_value=2, max_value=20, value=5, step=1)

            with col2:
                error_rate = st.slider("Error rate:", min_value=0.05, max_value=0.5, value=0.2, step=0.05)

                col_a, col_b, col_c = st.columns(3)
                with col_a:
                    missing_ratio = st.slider("Missing values ratio:", min_value=0.0, max_value=1.0, value=0.33,
                                              step=0.01)
                with col_b:
                    outlier_ratio = st.slider("Outlier values ratio:", min_value=0.0, max_value=1.0, value=0.33,
                                              step=0.01)
                with col_c:
                    noise_ratio = st.slider("Noise ratio:", min_value=0.0, max_value=1.0, value=0.34, step=0.01)

                # å½’ä¸€åŒ–æ¯”ä¾‹æ€»å’Œä¸º1
                total = missing_ratio + outlier_ratio + noise_ratio
                missing_ratio = missing_ratio / total
                outlier_ratio = outlier_ratio / total
                noise_ratio = noise_ratio / total

            st.info(
                f"Normalized ratios: Missing={missing_ratio:.2f}, Outliers={outlier_ratio:.2f}, Noise={noise_ratio:.2f}")

            # ç”Ÿæˆç¤ºä¾‹æ•°æ®çš„æŒ‰é’®
            if st.button("Generate Example Data"):
                with st.spinner("Generating example data..."):
                    # ç”Ÿæˆå¹²å‡€æ•°æ®
                    clean_df = generate_clean_data(
                        task_type=task_type,
                        n_samples=n_samples,
                        n_features=n_features
                    )

                    # åˆ›å»ºé”™è¯¯æ³¨å…¥å™¨
                    injector = ErrorInjector(clean_df)

                    # æ³¨å…¥é”™è¯¯
                    missing_err_rate = error_rate * missing_ratio
                    outlier_err_rate = error_rate * outlier_ratio
                    noise_err_rate = error_rate * noise_ratio

                    df_with_errors = injector.inject_missing_values(error_rate=missing_err_rate)
                    df_with_errors = injector.inject_outliers(error_rate=outlier_err_rate)
                    df_with_errors = injector.inject_noise(error_rate=noise_err_rate)

                    error_locations = injector.error_locations

                    # ä¿å­˜åˆ°session state
                    st.session_state['example_clean_df'] = clean_df
                    st.session_state['example_df_with_errors'] = df_with_errors
                    st.session_state['example_injector'] = injector

                    st.success(f"Successfully generated example data with {len(error_locations)} errors.")

                    # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
                    col1, col2 = st.columns(2)
                    with col1:
                        st.write("Clean Data Preview:")
                        st.dataframe(clean_df.head(5))
                    with col2:
                        st.write("Data with Errors Preview:")
                        st.dataframe(df_with_errors.head(5))

                    # å¯è§†åŒ–ç”Ÿæˆçš„æ•°æ®
                    if st.checkbox("Show Data Visualization", value=True):
                       # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
                        col1, col2 = st.columns(2)
                        with col1:
                            st.write("Clean Data:")
                            fig = visualize_data(clean_df, task_type,
                                                 f"Generated Clean Data ({error_rate:.2f})", return_fig=True)
                            st.pyplot(fig)
                        with col2:
                            st.write("Data with Errors:")
                            fig = visualize_data(df_with_errors, task_type,
                                                 f"Generated Data with Errors ({error_rate:.2f})", return_fig=True)
                            st.pyplot(fig)

            # å¦‚æžœå·²ç”Ÿæˆç¤ºä¾‹æ•°æ®ï¼Œæä¾›è¿è¡Œæ¯”è¾ƒçš„æŒ‰é’®
            if 'example_df_with_errors' in st.session_state:
                if st.button("Run Comparison with Generated Data"):
                    df_with_errors = st.session_state['example_df_with_errors']
                    clean_df = st.session_state['example_clean_df']
                    injector = st.session_state['example_injector']
                    error_locations = injector.error_locations
                    run_comparison = True

        # åªæœ‰å½“ç”¨æˆ·ç‚¹å‡»äº†ç›¸åº”æŒ‰é’®åŽæ‰è¿è¡Œç­–ç•¥æ¯”è¾ƒ
        if run_comparison and df_with_errors is not None and clean_df is not None and error_locations is not None:
            with st.spinner("Computing strategy comparisons..."):
                # ä»Žsession_stateèŽ·å–çŽ¯å¢ƒå’Œä»£ç†
                env = st.session_state['env']
                agent = st.session_state['agent']
                task_type = st.session_state.get('task_type', 'classification')

                # å¦‚æžœä½¿ç”¨æ–°æ•°æ®ï¼Œæ›´æ–°çŽ¯å¢ƒ
                if data_source != "Use data from previous steps":
                    # ä¸ºæ–°æ•°æ®åˆ›å»ºçŽ¯å¢ƒ
                    if task_type == 'classification':
                        ml_model = RandomForestClassifier(n_estimators=10, random_state=42)
                    else:
                        ml_model = RandomForestRegressor(n_estimators=10, random_state=42)

                    model_type = st.session_state.get('model_type', 'random_forest')
                    env = DataCleaningEnv(df_with_errors, ml_model, error_locations,
                                          task_type=task_type, model_type=model_type)

                # è®¾ç½®è¯„ä¼°å™¨
                X = clean_df.drop('target', axis=1)
                y = clean_df['target']
                X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)
                model_type = st.session_state.get('model_type', 'random_forest')
                evaluator = ModelEvaluator(X_train, y_train, X_val, y_val,
                                           task_type=task_type, model_type=model_type)

                # è¯„ä¼°ç­–ç•¥
                strat_perf = evaluate_strategies(task_type, df_with_errors, error_locations,
                                                 evaluator, env, agent)

                # å°†ç­–ç•¥æ€§èƒ½å­˜å‚¨åˆ°session_state
                st.session_state['strategy_performance'] = strat_perf

                # æ”¶é›†åŠ¨ä½œç»Ÿè®¡
                env.df = df_with_errors.copy()
                env.current_error_idx = 0
                actions = []
                feature_importances = []
                error_types = []

                state = env.reset()
                for _ in range(len(error_locations)):
                    action = agent.act(state)
                    actions.append(action)
                    etype, f_imp, _, _, _ = state
                    feature_importances.append(f_imp)
                    error_types.append(etype)
                    next_state, _, done, _ = env.step(action)
                    state = next_state
                    if done:
                        break
                col1, col2 = st.columns([1, 2])
                with col1:
                    # ç”Ÿæˆå¹¶æ˜¾ç¤ºç­–ç•¥å¯¹æ¯”å›¾
                    st.subheader("Strategy Performance Comparison")

                    # åˆ›å»ºæ€§èƒ½æ¯”è¾ƒå›¾
                    fig, ax = plt.subplots(figsize=(5, 6))

                    strategies = ['Do Nothing', 'Delete All', 'Repair All', 'RL Optimal']

                    if task_type == 'classification':
                        metrics = [strat_perf['do_nothing']['accuracy'],
                                   strat_perf['delete_all']['accuracy'],
                                   strat_perf['repair_all']['accuracy'],
                                   strat_perf['RL_optimal']['accuracy']]
                        metric_name = "Accuracy"
                    else:
                        metrics = [strat_perf['do_nothing']['r2_score'],
                                   strat_perf['delete_all']['r2_score'],
                                   strat_perf['repair_all']['r2_score'],
                                   strat_perf['RL_optimal']['r2_score']]
                        metric_name = "RÂ² Score"

                    # åˆ›å»ºæ¡å½¢å›¾
                    bars = ax.bar(strategies, metrics, color=['gray', 'red', 'blue', 'green'])

                    # æ·»åŠ æ•°æ®æ ‡ç­¾
                    for bar in bars:
                        height = bar.get_height()
                        ax.text(bar.get_x() + bar.get_width() / 2., height + 0.01,
                                f'{height:.3f}', ha='center', va='bottom')

                    ax.set_ylabel(metric_name)
                    ax.set_title('Strategy Performance Comparison')
                    ax.set_ylim(0, max(metrics) * 1.1)

                    plt.tight_layout()
                    st.pyplot(fig)

                # å¦‚æžœæ˜¯æ–°ç”Ÿæˆæˆ–ä¸Šä¼ çš„æ•°æ®ï¼Œä¿å­˜å›¾åƒ
                if data_source != "Use data from previous steps":
                    source_label = "custom" if data_source == "Upload new test data" else "generated"
                    fig_filename = f"strategy_comparison_{task_type}_{source_label}_data.png"
                    fig.savefig(fig_filename)
                    st.success(f"Saved strategy comparison as {fig_filename}")

                # è®¡ç®—åŠ¨ä½œç»Ÿè®¡
                actions = np.array(actions)
                feature_importances = np.array(feature_importances)
                error_types = np.array(error_types)

                action_counts = {
                    'no_action': np.sum(actions == 0),
                    'repair': np.sum(actions == 1),
                    'delete': np.sum(actions == 2)
                }

                action_rates = {
                    'no_action': np.sum(actions == 0) / len(actions),
                    'repair': np.sum(actions == 1) / len(actions),
                    'delete': np.sum(actions == 2) / len(actions)
                }

                high_importance_mask = feature_importances > 0.5
                low_importance_mask = feature_importances <= 0.5
                missing_mask = np.array(error_types) == 0
                outlier_mask = np.array(error_types) == 1

                high_importance_actions = {
                    'no_action': np.sum(actions[high_importance_mask] == 0) / max(1, np.sum(high_importance_mask)),
                    'repair': np.sum(actions[high_importance_mask] == 1) / max(1, np.sum(high_importance_mask)),
                    'delete': np.sum(actions[high_importance_mask] == 2) / max(1, np.sum(high_importance_mask))
                }

                low_importance_actions = {
                    'no_action': np.sum(actions[low_importance_mask] == 0) / max(1, np.sum(low_importance_mask)),
                    'repair': np.sum(actions[low_importance_mask] == 1) / max(1, np.sum(low_importance_mask)),
                    'delete': np.sum(actions[low_importance_mask] == 2) / max(1, np.sum(low_importance_mask))
                }

                missing_actions = {
                    'no_action': np.sum(actions[missing_mask] == 0) / max(1, np.sum(missing_mask)),
                    'repair': np.sum(actions[missing_mask] == 1) / max(1, np.sum(missing_mask)),
                    'delete': np.sum(actions[missing_mask] == 2) / max(1, np.sum(missing_mask))
                }

                outlier_actions = {
                    'no_action': np.sum(actions[outlier_mask] == 0) / max(1, np.sum(outlier_mask)),
                    'repair': np.sum(actions[outlier_mask] == 1) / max(1, np.sum(outlier_mask)),
                    'delete': np.sum(actions[outlier_mask] == 2) / max(1, np.sum(outlier_mask))
                }

                # å°†åŠ¨ä½œç»Ÿè®¡å­˜å‚¨åˆ°session_state
                st.session_state['action_stats'] = {
                    'action_counts': action_counts,
                    'action_rates': action_rates,
                    'high_importance_actions': high_importance_actions,
                    'low_importance_actions': low_importance_actions,
                    'missing_actions': missing_actions,
                    'outlier_actions': outlier_actions
                }

                st.session_state['step3_complete'] = True

                # æ˜¾ç¤ºåŠ¨ä½œåˆ†å¸ƒ
                st.subheader("Action Distribution Statistics")

                # ç»¼åˆåŠ¨ä½œåˆ†å¸ƒå›¾ - ç´§å‡‘ç‰ˆ
                fig, axs = plt.subplots(1, 4, figsize=(12, 4))
                action_stats = st.session_state['action_stats']
                # 1. æ•´ä½“åŠ¨ä½œè®¡æ•°
                action_names = ['No Action', 'Repair', 'Delete']
                action_values = [action_stats['action_counts']['no_action'],
                                 action_stats['action_counts']['repair'],
                                 action_stats['action_counts']['delete']]

                bars = axs[0].bar(action_names, action_values, width=0.5, color=['gray', 'blue', 'red'])
                axs[0].set_xlabel("Action Type")
                axs[0].set_ylabel("Count")
                axs[0].set_title("Overall Action Distribution")

                for i, count in enumerate(action_values):
                    axs[0].text(i, count + 0.5, str(count), ha='center')

                # 2. æŒ‰ç‰¹å¾é‡è¦æ€§ - é«˜é‡è¦æ€§
                high_imp_values = [action_stats['high_importance_actions']['no_action'],
                                   action_stats['high_importance_actions']['repair'],
                                   action_stats['high_importance_actions']['delete']]

                bars = axs[1].bar(action_names, high_imp_values, width=0.5, color=['gray', 'blue', 'red'])
                axs[1].set_xlabel("Action Type")
                axs[1].set_ylabel("Proportion")
                axs[1].set_title("Actions for High Importance Features")
                axs[1].set_ylim(0, 1)

                for i, val in enumerate(high_imp_values):
                    axs[1].text(i, val + 0.02, f"{val:.2f}", ha='center')

                # 3. æŒ‰é”™è¯¯ç±»åž‹ - ç¼ºå¤±å€¼
                missing_values = [action_stats['missing_actions']['no_action'],
                                  action_stats['missing_actions']['repair'],
                                  action_stats['missing_actions']['delete']]

                bars = axs[2].bar(action_names, missing_values, width=0.5, color=['gray', 'blue', 'red'])
                axs[2].set_xlabel("Action Type")
                axs[2].set_ylabel("Proportion")
                axs[2].set_title("Actions for Missing Values")
                axs[2].set_ylim(0, 1)

                for i, val in enumerate(missing_values):
                    axs[2].text(i, val + 0.02, f"{val:.2f}", ha='center')

                # 4. æŒ‰é”™è¯¯ç±»åž‹ - å¼‚å¸¸å€¼
                outlier_values = [action_stats['outlier_actions']['no_action'],
                                  action_stats['outlier_actions']['repair'],
                                  action_stats['outlier_actions']['delete']]

                bars = axs[3].bar(action_names, outlier_values, width=0.3, color=['gray', 'blue', 'red'])
                axs[3].set_xlabel("Action Type")
                axs[3].set_ylabel("Proportion")
                axs[3].set_title("Actions for Outliers/Noise")
                axs[3].set_ylim(0, 1)

                for i, val in enumerate(outlier_values):
                    axs[3].text(i, val + 0.02, f"{val:.2f}", ha='center')

                plt.tight_layout()
                st.pyplot(fig)

                # å¦‚æžœæ˜¯æ–°ç”Ÿæˆæˆ–ä¸Šä¼ çš„æ•°æ®ï¼Œä¿å­˜å›¾åƒ
                if data_source != "Use data from previous steps":
                    source_label = "custom" if data_source == "Upload new test data" else "generated"
                    fig_filename = f"action_distribution_{task_type}_{source_label}_data.png"
                    fig.savefig(fig_filename)
                    st.success(f"Saved action distribution as {fig_filename}")
                with col2:
                    # ç‰¹æ€§é‡è¦æ€§æ¯”è¾ƒå›¾ - é™„åŠ å›¾ï¼Œæ›´ç›´è§‚æ˜¾ç¤ºå·®å¼‚
                    st.subheader("Action Strategy by Feature Importance & Error Type")

                    fig, ax = plt.subplots(figsize=(10, 6))
                    x = np.arange(3)
                    width = 0.2

                    ax.bar(x - width * 1.5, high_imp_values, width, label='High Importance', color='darkblue')
                    ax.bar(x - width * 0.5, low_importance_actions.values(), width, label='Low Importance', color='skyblue')
                    ax.bar(x + width * 0.5, missing_values, width, label='Missing Values', color='orange')
                    ax.bar(x + width * 1.5, outlier_values, width, label='Outliers', color='red')

                    ax.set_xticks(x)
                    ax.set_xticklabels(action_names)
                    ax.set_ylabel('Proportion')
                    ax.set_title('Action Strategy Comparison')
                    ax.legend()
                    ax.grid(True, linestyle='--', alpha=0.7)

                    plt.tight_layout()
                    st.pyplot(fig)

                if data_source != "Use data from previous steps":
                    source_label = "custom" if data_source == "Upload new test data" else "generated"
                    fig_filename = f"action_strategy_{task_type}_{source_label}_data.png"
                    fig.savefig(fig_filename)
                    st.success(f"Saved action strategy as {fig_filename}")

                # æ˜¾ç¤ºç»“æžœæ‘˜è¦
                st.subheader(f"Results Summary - {data_source.replace('Use ', '').title()}")

                # åˆ›å»ºæ€§èƒ½æ‘˜è¦è¡¨
                if task_type == 'classification':
                    perf_df = pd.DataFrame({
                        'Strategy': ['Do Nothing', 'Delete All', 'Repair All', 'RL Optimal'],
                        'Accuracy': [strat_perf['do_nothing']['accuracy'],
                                     strat_perf['delete_all']['accuracy'],
                                     strat_perf['repair_all']['accuracy'],
                                     strat_perf['RL_optimal']['accuracy']],
                        'Precision': [strat_perf['do_nothing'].get('precision', np.nan),
                                      strat_perf['delete_all'].get('precision', np.nan),
                                      strat_perf['repair_all'].get('precision', np.nan),
                                      strat_perf['RL_optimal'].get('precision', np.nan)],
                        'Recall': [strat_perf['do_nothing'].get('recall', np.nan),
                                   strat_perf['delete_all'].get('recall', np.nan),
                                   strat_perf['repair_all'].get('recall', np.nan),
                                   strat_perf['RL_optimal'].get('recall', np.nan)],
                        'F1 Score': [strat_perf['do_nothing'].get('f1_score', np.nan),
                                     strat_perf['delete_all'].get('f1_score', np.nan),
                                     strat_perf['repair_all'].get('f1_score', np.nan),
                                     strat_perf['RL_optimal'].get('f1_score', np.nan)]
                    })
                else:
                    perf_df = pd.DataFrame({
                        'Strategy': ['Do Nothing', 'Delete All', 'Repair All', 'RL Optimal'],
                        'RÂ² Score': [strat_perf['do_nothing']['r2_score'],
                                     strat_perf['delete_all']['r2_score'],
                                     strat_perf['repair_all']['r2_score'],
                                     strat_perf['RL_optimal']['r2_score']],
                        'MSE': [strat_perf['do_nothing'].get('mse', np.nan),
                                strat_perf['delete_all'].get('mse', np.nan),
                                strat_perf['repair_all'].get('mse', np.nan),
                                strat_perf['RL_optimal'].get('mse', np.nan)],
                        'MAE': [strat_perf['do_nothing'].get('mae', np.nan),
                                strat_perf['delete_all'].get('mae', np.nan),
                                strat_perf['repair_all'].get('mae', np.nan),
                                strat_perf['RL_optimal'].get('mae', np.nan)]
                    })

                # æ ¼å¼åŒ–æ•°å€¼
                for col in perf_df.columns:
                    if col != 'Strategy':
                        perf_df[col] = perf_df[col].apply(lambda x: f"{x:.3f}" if not pd.isna(x) else "N/A")

                st.write("Performance Metrics:")
                st.table(perf_df)

                # åˆ›å»ºåŠ¨ä½œåˆ†å¸ƒæ‘˜è¦
                action_df = pd.DataFrame({
                    'Action Type': ['No Action', 'Repair', 'Delete'],
                    'Count': [action_counts['no_action'], action_counts['repair'], action_counts['delete']],
                    'Percentage': [f"{action_rates['no_action'] * 100:.1f}%",
                                   f"{action_rates['repair'] * 100:.1f}%",
                                   f"{action_rates['delete'] * 100:.1f}%"]
                })

                st.write("Action Distribution:")
                st.table(action_df)

                # æ·»åŠ æŒ‰é’®è·³è½¬åˆ°Step 4
                if st.button("Proceed to Model Comparison"):
                    st.rerun()

        # å¦‚æžœæ­¥éª¤å®Œæˆä½†æœªé€‰æ‹©è¿è¡Œï¼Œæé†’ç”¨æˆ·é€‰æ‹©åŠ¨ä½œ
        elif st.session_state.get('step2_complete', False) and not run_comparison:
            if data_source == "Use data from previous steps":
                st.info("Click 'Run Comparison with Existing Data' to continue.")
            elif data_source == "Upload new test data":
                if 'test_error_locations' not in st.session_state:
                    st.info("Please upload test data and clean data, then detect error locations.")
                else:
                    st.info("Click 'Run Comparison with Uploaded Data' to continue.")
            elif data_source == "Generate example test data":
                if 'example_df_with_errors' not in st.session_state:
                    st.info("Click 'Generate Example Data' to create test data.")
                else:
                    st.info("Click 'Run Comparison with Generated Data' to continue.")

elif page == "Step 4: Model Comparison":
    st.header("Step 4: Multi-Model Comparison & Tolerance Analysis")

    if not st.session_state.get('step3_complete', False):
        st.warning("Please complete Step 3 first.")
        if st.button("Go to Step 3"):
            st.rerun()
    else:
        # æ¨¡åž‹é€‰æ‹©éƒ¨åˆ†
        st.subheader("Select Models for Comparison")
        task_type = st.session_state.get('task_type', 'classification')
        error_rate = st.session_state['error_rate']

        if task_type == 'classification':
            available_models = ["random_forest", "svm", "logistic_regression"]
        else:
            available_models = ["random_forest", "svm", "linear_regression"]

        selected_models = st.multiselect(
            "Select models to compare:",
            available_models,
            default=[st.session_state.get('model_type', available_models[0])]
        )

        # é€‰æ‹©æ˜¯ä½¿ç”¨çŽ°æœ‰ç­–ç•¥è¿˜æ˜¯é‡æ–°è®­ç»ƒ
        use_policy = st.radio(
            "Use existing RL policy or retrain for each model?",
            ["Use existing policy", "Retrain RL for each model"]
        )

        if st.button("Run Model Comparison"):
            if not selected_models:
                st.error("Please select at least one model.")
            else:
                with st.spinner("Running model comparison..."):
                    # ä»Žsession_stateèŽ·å–æ•°æ®
                    clean_df = st.session_state['clean_df']
                    df_with_errors = st.session_state['df_with_errors']
                    injector = st.session_state['injector']  # è¿™é‡Œè¦ç¡®ä¿injectoråœ¨session_stateä¸­
                    agent = st.session_state['agent']

                    # å‡†å¤‡è¯„ä¼°æ•°æ®
                    X = clean_df.drop('target', axis=1)
                    y = clean_df['target']
                    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)

                    # åˆå§‹åŒ–ç»“æžœå­˜å‚¨
                    model_results = {}

                    for model_type in selected_models:
                        st.text(f"Processing model: {model_type}")

                        # æ£€æŸ¥æ˜¯å¦å·²æœ‰è®­ç»ƒå¥½çš„æ¨¡åž‹
                        model_exists = check_model_exists(model_type, task_type, error_rate)

                        # ä¸ºè¯¥æ¨¡åž‹åˆå§‹åŒ–è¯„ä¼°å™¨
                        evaluator = ModelEvaluator(X_train, y_train, X_val, y_val,
                                                   task_type=task_type, model_type=model_type)

                        if use_policy == "Use existing policy" or model_exists:
                            # ä½¿ç”¨å·²è®­ç»ƒçš„ä»£ç†ï¼ˆæˆ–åŠ è½½çŽ°æœ‰æ¨¡åž‹ï¼‰
                            env = st.session_state['env']
                            env.model_type = model_type  # æ›´æ–°çŽ¯å¢ƒä¸­çš„æ¨¡åž‹ç±»åž‹

                            if model_exists and model_type != st.session_state.get('model_type'):
                                # å¦‚æžœå­˜åœ¨é’ˆå¯¹å½“å‰æ¨¡åž‹çš„é¢„è®­ç»ƒæ¨¡åž‹ï¼ŒåŠ è½½å®ƒ
                                current_agent = DQNAgent(state_size=5, action_size=3)
                                model = load_model(model_type, task_type, error_rate)
                                if model:
                                    current_agent.model = model
                                    agent = current_agent
                        else:
                            # ä¸ºè¯¥æ¨¡åž‹ç±»åž‹åˆ›å»ºæ–°çŽ¯å¢ƒ
                            if task_type == 'classification':
                                if model_type == 'svm':
                                    ml_model = SVC(probability=True, random_state=42)
                                elif model_type == 'logistic_regression':
                                    ml_model = LogisticRegression(random_state=42, max_iter=1000)
                                else:
                                    ml_model = RandomForestClassifier(n_estimators=10, random_state=42)
                            else:
                                if model_type == 'svm':
                                    ml_model = SVR()
                                elif model_type == 'linear_regression':
                                    ml_model = LinearRegression()
                                else:
                                    ml_model = RandomForestRegressor(n_estimators=10, random_state=42)

                            # åˆ›å»ºæ–°çŽ¯å¢ƒ
                            env = DataCleaningEnv(df_with_errors.copy(), ml_model, injector.error_locations,
                                                  task_type=task_type, model_type=model_type)

                            # è®­ç»ƒæ–°ä»£ç†
                            new_agent = DQNAgent(state_size=5, action_size=3)

                            # å¿«é€Ÿè®­ç»ƒï¼ˆè¾ƒå°‘è½®æ¬¡ï¼‰
                            n_quick_episodes = 20
                            for e in range(n_quick_episodes):
                                state = env.reset()
                                for _ in range(len(injector.error_locations)):
                                    action = new_agent.act(state)
                                    next_state, reward, done, _ = env.step(action)
                                    new_agent.remember(state, action, reward, next_state, done)
                                    state = next_state
                                    if done:
                                        break
                                new_agent.replay(batch_size=32)

                            # ä½¿ç”¨æ–°è®­ç»ƒçš„ä»£ç†
                            agent = new_agent

                            # ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡åž‹
                            save_model(agent, model_type, task_type, error_rate)

                        # ç”¨å½“å‰æ¨¡åž‹è¯„ä¼°ç­–ç•¥
                        strat_perf = evaluate_strategies(task_type, df_with_errors,
                                                         injector.error_locations, evaluator, env, agent)

                        # æ”¶é›†åŠ¨ä½œç»Ÿè®¡
                        env.df = df_with_errors.copy()
                        env.current_error_idx = 0
                        actions = []

                        state = env.reset()
                        for _ in range(len(injector.error_locations)):
                            action = agent.act(state)
                            actions.append(action)
                            next_state, _, done, _ = env.step(action)
                            state = next_state
                            if done:
                                break

                        actions = np.array(actions)
                        action_counts = {
                            'no_action': np.sum(actions == 0),
                            'repair': np.sum(actions == 1),
                            'delete': np.sum(actions == 2)
                        }

                        # å­˜å‚¨è¯¥æ¨¡åž‹çš„ç»“æžœ
                        model_results[model_type] = {
                            'strategy_performance': strat_perf,
                            'action_counts': action_counts
                        }

                    # å°†æ¨¡åž‹æ¯”è¾ƒç»“æžœå­˜å‚¨åˆ°session_state
                    st.session_state['model_comparison'] = model_results
                    st.session_state['step4_complete'] = True

                st.success("Model comparison complete!")

        # å¦‚æžœæœ‰æ¨¡åž‹æ¯”è¾ƒç»“æžœï¼Œæ˜¾ç¤ºç»“æžœ
        if st.session_state.get('model_comparison'):
            st.subheader("Model Comparison Results")

            model_results = st.session_state['model_comparison']

            # æ€§èƒ½æŒ‡æ ‡æ¯”è¾ƒ
            st.write("Performance Metrics Across Models:")

            # ä¼˜åŒ–Step 4çš„æ¨¡åž‹æ¯”è¾ƒå›¾è¡¨
            if task_type == 'classification':
                # è®¡ç®—yè½´çš„åˆç†èŒƒå›´
                all_accs = []
                for model_name in model_results.keys():
                    strat_perf = model_results[model_name]['strategy_performance']
                    all_accs.extend([
                        strat_perf['do_nothing']["accuracy"],
                        strat_perf['delete_all']["accuracy"],
                        strat_perf['repair_all']["accuracy"],
                        strat_perf['RL_optimal']["accuracy"]
                    ])

                min_acc = max(0.5, min(all_accs) - 0.05)  # ä»Ž0.5å¼€å§‹ä»¥çªå‡ºå·®å¼‚
                max_acc = min(1, max(all_accs) + 0.05)

                # åˆ›å»ºä¸€ä¸ªç´§å‡‘çš„å‡†ç¡®çŽ‡æ¯”è¾ƒå›¾
                col1, col2 = st.columns(2)

                with col1:
                    fig, ax = plt.subplots(figsize=(6, 4))
                    bar_width = 0.15
                    index = np.arange(4)

                    for i, model_name in enumerate(model_results.keys()):
                        strat_perf = model_results[model_name]['strategy_performance']
                        accuracies = [strat_perf['do_nothing']["accuracy"],
                                      strat_perf['delete_all']["accuracy"],
                                      strat_perf['repair_all']["accuracy"],
                                      strat_perf['RL_optimal']["accuracy"]]

                        bars = ax.bar(index + i * bar_width, accuracies, bar_width, label=model_name)

                        for j, bar in enumerate(bars):
                            height = bar.get_height()
                            ax.text(bar.get_x() + bar.get_width() / 2., height + 0.005,
                                    f'{accuracies[j]:.3f}', ha='center', va='bottom',
                                    rotation=45, fontsize=8)

                    ax.set_xlabel('Strategy')
                    ax.set_ylabel('Accuracy')
                    ax.set_title('Model Accuracy by Strategy')
                    ax.set_xticks(index + bar_width * (len(model_results) - 1) / 2)
                    ax.set_xticklabels(['Do Nothing', 'Delete All', 'Repair All', 'RL Optimal'], rotation=20)
                    ax.legend(loc='lower right', fontsize=8)
                    ax.set_ylim(min_acc, max_acc)
                    ax.yaxis.grid(True, linestyle='--', alpha=0.7)
                    plt.tight_layout()
                    st.pyplot(fig)

                with col2:
                    # æŠ˜çº¿å›¾æ¯”è¾ƒ
                    fig, ax = plt.subplots(figsize=(6, 4))
                    strategies = ['Do Nothing', 'Delete All', 'Repair All', 'RL Optimal']

                    for model_name in model_results.keys():
                        strat_perf = model_results[model_name]['strategy_performance']
                        accuracies = [strat_perf['do_nothing']["accuracy"],
                                      strat_perf['delete_all']["accuracy"],
                                      strat_perf['repair_all']["accuracy"],
                                      strat_perf['RL_optimal']["accuracy"]]

                        ax.plot(strategies, accuracies, 'o-', linewidth=2, markersize=6, label=model_name)

                        for i, acc in enumerate(accuracies):
                            ax.text(i, acc + 0.005, f'{acc:.3f}', ha='center', fontsize=8)

                    ax.set_xlabel('Strategy')
                    ax.set_ylabel('Accuracy')
                    ax.set_title('Accuracy Trend Across Strategies')
                    ax.legend(fontsize=8)
                    ax.grid(True, linestyle='--', alpha=0.7)
                    ax.set_ylim(min_acc, max_acc)
                    plt.tight_layout()
                    st.pyplot(fig)

            # åŠ¨ä½œåˆ†å¸ƒæ¯”è¾ƒ
            # ä¿®æ”¹åŠ¨ä½œåˆ†å¸ƒæ¯”è¾ƒå›¾ - æ›´ç´§å‡‘çš„å¸ƒå±€
            # åˆ›å»ºä¸€ä¸ªç»„åˆå›¾è¡¨æ¥æ˜¾ç¤ºåŠ¨ä½œåˆ†å¸ƒå’Œç­–ç•¥æ¯”è¾ƒ
            st.subheader("Action Distribution & Model Strategy Analysis")

            # ä¼˜åŒ–åŠ¨ä½œåˆ†å¸ƒæ¯”è¾ƒ
            col1, col2 = st.columns([3, 2])

            with col1:
                # å·¦ä¾§ - æ¡å½¢å›¾æ¯”è¾ƒå„æ¨¡åž‹çš„åŠ¨ä½œåˆ†å¸ƒ
                fig, ax = plt.subplots(figsize=(8, 5))
                bar_width = 0.15
                index = np.arange(3)  # 3ç§åŠ¨ä½œç±»åž‹

                for i, model_name in enumerate(model_results.keys()):
                    action_counts = model_results[model_name]['action_counts']
                    total = sum(action_counts.values())
                    counts_pct = [action_counts['no_action'] / total,
                                  action_counts['repair'] / total,
                                  action_counts['delete'] / total]

                    ax.bar(index + i * bar_width, counts_pct, bar_width, label=model_name)

                    for j, pct in enumerate(counts_pct):
                        ax.text(index[j] + i * bar_width, pct + 0.02, f"{pct:.0%}",
                                ha='center', va='bottom', fontsize=8, rotation=45)

                ax.set_xlabel('Action Type')
                ax.set_ylabel('Proportion')
                ax.set_title('Action Distribution by Model')
                ax.set_xticks(index + bar_width * (len(model_results) - 1) / 2)
                ax.set_xticklabels(['No Action', 'Repair', 'Delete'])
                ax.legend()
                ax.set_ylim(0, 1)
                ax.grid(True, linestyle='--', alpha=0.3)
                plt.tight_layout()
                st.pyplot(fig)

            with col2:
                # å³ä¾§ - é¥¼å›¾æ˜¾ç¤ºæ•´ä½“çš„åŠ¨ä½œåå¥½
                fig, ax = plt.subplots(figsize=(5, 5))
                total_actions = {'no_action': 0, 'repair': 0, 'delete': 0}
                for model_name in model_results.keys():
                    for action_type in total_actions:
                        total_actions[action_type] += model_results[model_name]['action_counts'][action_type]

                total_sum = sum(total_actions.values())
                action_labels = ['No Action', 'Repair', 'Delete']
                sizes = [total_actions['no_action'] / total_sum,
                         total_actions['repair'] / total_sum,
                         total_actions['delete'] / total_sum]
                colors = ['#4878d0', '#ee854a', '#d64646']
                explode = (0.05, 0, 0)  # çªå‡º"No Action"

                wedges, texts, autotexts = ax.pie(sizes, explode=explode, labels=action_labels,
                                                  colors=colors, autopct='%1.1f%%',
                                                  shadow=True, startangle=90,
                                                  textprops={'fontsize': 9})
                for autotext in autotexts:
                    autotext.set_fontsize(8)
                ax.axis('equal')
                ax.set_title('Overall Action Distribution')
                plt.tight_layout()
                st.pyplot(fig)

            # å®¹å¿åº¦é˜ˆå€¼åˆ†æžéƒ¨åˆ†
            # æ”¹è¿›å®¹å¿åº¦é˜ˆå€¼åˆ†æž - ä½¿ç”¨å¯è§†åŒ–ä»£æ›¿é•¿ç¯‡æ–‡å­—è¯´æ˜Ž
            st.subheader("Data Quality Tolerance Analysis")

            # ä½¿ç”¨çƒ­å›¾å’Œé›·è¾¾å›¾æ¥å¯è§†åŒ–å®¹å¿åº¦é˜ˆå€¼
            tolerance_data = {
                'overall_tolerance': st.session_state['error_rate'] * 0.8,
                'repair_vs_delete': st.session_state['error_rate'] * 0.6,
                'key_feature_tolerance': st.session_state['error_rate'] * 0.7,
                'non_key_feature_tolerance': st.session_state['error_rate'] * 0.9,
                'missing_tolerance': st.session_state['error_rate'] * 0.75,
                'outlier_tolerance': st.session_state['error_rate'] * 0.65
            }

            # åˆ›å»ºä¸€ä¸ªçƒ­å›¾æ˜¾ç¤ºä¸åŒé˜ˆå€¼
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

            # å·¦ä¾§ï¼šçƒ­å›¾
            tolerance_df = pd.DataFrame({
                'Threshold Type': ['Overall', 'Repair vs Delete', 'Key Features', 'Non-key Features', 'Missing Values',
                                   'Outliers'],
                'Tolerance Value': [tolerance_data['overall_tolerance'],
                                    tolerance_data['repair_vs_delete'],
                                    tolerance_data['key_feature_tolerance'],
                                    tolerance_data['non_key_feature_tolerance'],
                                    tolerance_data['missing_tolerance'],
                                    tolerance_data['outlier_tolerance']]
            })

            # å°†æ•°æ®è½¬ä¸ºçƒ­å›¾æ ¼å¼
            heatmap_data = tolerance_df.set_index('Threshold Type')
            sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='YlOrRd', ax=ax1)
            ax1.set_title('Data Quality Tolerance Thresholds')
            ax1.set_ylabel('')

            # å³ä¾§ï¼šå®¹å¿åº¦æ¯”è¾ƒå›¾
            categories = ['Overall', 'Repair/Delete', 'Key Features',
                          'Non-key Features', 'Missing', 'Outliers']
            values = [tolerance_data['overall_tolerance'],
                      tolerance_data['repair_vs_delete'],
                      tolerance_data['key_feature_tolerance'],
                      tolerance_data['non_key_feature_tolerance'],
                      tolerance_data['missing_tolerance'],
                      tolerance_data['outlier_tolerance']]

            # å…³é—­å³ä¾§çš„è½´çº¿
            ax2.spines['top'].set_visible(False)
            ax2.spines['right'].set_visible(False)
            ax2.spines['bottom'].set_visible(False)
            ax2.spines['left'].set_visible(False)

            # åˆ›å»ºæ¡å½¢å›¾
            bars = ax2.barh(categories, values, color=plt.cm.YlOrRd(np.linspace(0.2, 0.8, len(categories))))
            ax2.set_xlim(0, 1)
            ax2.set_xlabel('Tolerance Threshold')
            ax2.set_title('Tolerance Threshold Analysis')
            ax2.grid(True, linestyle='--', alpha=0.3)

            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, v in enumerate(values):
                ax2.text(v + 0.02, i, f'{v:.3f}', va='center')

            plt.tight_layout()
            st.pyplot(fig)

            # æ”¹è¿›æ¨¡åž‹æ€§èƒ½vsé”™è¯¯çŽ‡çš„å¯è§†åŒ–
            fig, ax = plt.subplots(figsize=(10, 6))

            # ä¸åŒé”™è¯¯çŽ‡ä¸‹çš„æ€§èƒ½
            error_rates = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]

            # èŽ·å–å½“å‰æ•°æ®èŒƒå›´
            all_perf_values = []

            for model_name in model_results.keys():
                current_perf = model_results[model_name]['strategy_performance']['RL_optimal']

                if task_type == 'classification':
                    base_perf = current_perf["accuracy"]
                else:
                    base_perf = current_perf["r2_score"]

                simulated_perf = [
                    min(1.0, base_perf * (1 + 0.1 * (1 - er))) if er < st.session_state['error_rate']
                    else base_perf * (1 - 0.2 * (er - st.session_state['error_rate']))
                    for er in error_rates
                ]
                all_perf_values.extend(simulated_perf)

            # è®¡ç®—åˆç†çš„yè½´èŒƒå›´
            min_perf = max(0, min(all_perf_values) - 0.05)
            max_perf = min(1, max(all_perf_values) + 0.05)

            for model_name in model_results.keys():
                current_perf = model_results[model_name]['strategy_performance']['RL_optimal']

                if task_type == 'classification':
                    base_perf = current_perf["accuracy"]
                    metric_name = "Accuracy"
                else:
                    base_perf = current_perf["r2_score"]
                    metric_name = "RÂ² Score"

                simulated_perf = [
                    min(1.0, base_perf * (1 + 0.1 * (1 - er))) if er < st.session_state['error_rate']
                    else base_perf * (1 - 0.2 * (er - st.session_state['error_rate']))
                    for er in error_rates
                ]

                ax.plot(error_rates, simulated_perf, 'o-', linewidth=2, markersize=8, label=model_name)

                # ç”¨æ¨ªè·¨çš„çº¿æ ‡è¯†å®¹å¿åº¦é˜ˆå€¼
                threshold_idx = next(
                    (i for i, er in enumerate(error_rates) if er > tolerance_data['overall_tolerance']), None)
                if threshold_idx is not None and threshold_idx > 0:
                    # æ’å€¼æ‰¾åˆ°é˜ˆå€¼å¯¹åº”çš„æ€§èƒ½ç‚¹
                    perf_at_threshold = np.interp(tolerance_data['overall_tolerance'],
                                                  [error_rates[threshold_idx - 1], error_rates[threshold_idx]],
                                                  [simulated_perf[threshold_idx - 1], simulated_perf[threshold_idx]])

                    ax.plot([tolerance_data['overall_tolerance'], tolerance_data['overall_tolerance']],
                            [min_perf, perf_at_threshold], ':', color='gray', alpha=0.5)

                    ax.plot([0, tolerance_data['overall_tolerance']],
                            [perf_at_threshold, perf_at_threshold], ':', color='gray', alpha=0.5)

                    ax.text(tolerance_data['overall_tolerance'] - 0.02, min_perf + 0.02,
                            f'Threshold\n{tolerance_data["overall_tolerance"]:.2f}',
                            ha='right', va='bottom', fontsize=8)

            # æ ‡è®°å½“å‰é”™è¯¯çŽ‡
            ax.axvline(x=st.session_state['error_rate'], color='r', linestyle='--',
                       label=f'Current Error Rate: {st.session_state["error_rate"]:.2f}')

            # æ·»åŠ æ€§èƒ½é˜ˆå€¼çº¿
            ax.axhline(y=0.7, color='k', linestyle=':', label='Performance Threshold (0.7)')

            ax.set_xlabel('Error Rate')
            ax.set_ylabel(metric_name)
            ax.set_title('Model Performance vs Error Rate with Tolerance Threshold')
            ax.legend(loc='best')
            ax.grid(True, linestyle='--', alpha=0.7)

            # è°ƒæ•´yè½´èŒƒå›´ä»¥çªå‡ºå·®å¼‚
            ax.set_ylim(min_perf, max_perf)

            plt.tight_layout()
            st.pyplot(fig)

            # æ·»åŠ æŒ‰é’®è·³è½¬åˆ°Step 5
            if st.button("Proceed to Export Results"):
                st.rerun()

elif page == "Step 5: Export Results":
    st.header("Step 5: Export Results & Analysis Report")

    if not st.session_state.get('step4_complete', False):
        st.warning("Please complete Step 4 first.")
        if st.button("Go to Step 4"):
            st.rerun()
    else:
        # åŸºæœ¬ä¿¡æ¯ - ä½¿ç”¨ç¾Žè§‚çš„å¡ç‰‡å¸ƒå±€
        col1, col2, col3 = st.columns(3)

        with col1:
            st.markdown(f"""
            <div style="background-color:#f0f2f6;padding:15px;border-radius:10px;text-align:center;">
                <h4 style="margin:0;color:#666;">Task Type</h4>
                <h2 style="margin:5px 0;color:#1f77b4;">{st.session_state.get('task_type', 'classification').upper()}</h2>
            </div>
            """, unsafe_allow_html=True)

        with col2:
            st.markdown(f"""
            <div style="background-color:#f0f2f6;padding:15px;border-radius:10px;text-align:center;">
                <h4 style="margin:0;color:#666;">Total Error Rate</h4>
                <h2 style="margin:5px 0;color:#1f77b4;">{st.session_state['error_rate']:.2f}</h2>
            </div>
            """, unsafe_allow_html=True)

        with col3:
            st.markdown(f"""
            <div style="background-color:#f0f2f6;padding:15px;border-radius:10px;text-align:center;">
                <h4 style="margin:0;color:#666;">Error Composition</h4>
                <div style="margin:5px 0;font-size:0.9em;">
                    <span style="color:#e74c3c;font-weight:bold;">Missing: {st.session_state['missing_ratio']:.2f}</span> | 
                    <span style="color:#3498db;font-weight:bold;">Outliers: {st.session_state['outlier_ratio']:.2f}</span> | 
                    <span style="color:#2ecc71;font-weight:bold;">Noise: {st.session_state['noise_ratio']:.2f}</span>
                </div>
            </div>
            """, unsafe_allow_html=True)

        # æ±‡æ€»æ€§èƒ½æ•°æ®å¹¶å¯è§†åŒ–
        st.subheader("Performance Analysis")

        model_results = st.session_state['model_comparison']
        task_type = st.session_state.get('task_type', 'classification')

        # åˆ›å»ºæ‘˜è¦æ•°æ®
        summary_data = []
        for model_name, results in model_results.items():
            strat_perf = results['strategy_performance']
            if task_type == 'classification':
                summary_data.append({
                    'Model': model_name,
                    'Do Nothing': strat_perf['do_nothing']['accuracy'],
                    'Delete All': strat_perf['delete_all']['accuracy'],
                    'Repair All': strat_perf['repair_all']['accuracy'],
                    'RL Optimal': strat_perf['RL_optimal']['accuracy'],
                    'Metric': 'Accuracy'
                })
            else:
                summary_data.append({
                    'Model': model_name,
                    'Do Nothing': strat_perf['do_nothing']['r2_score'],
                    'Delete All': strat_perf['delete_all']['r2_score'],
                    'Repair All': strat_perf['repair_all']['r2_score'],
                    'RL Optimal': strat_perf['RL_optimal']['r2_score'],
                    'Metric': 'RÂ² Score'
                })

        # å°†æ•°æ®è½¬ä¸ºDataFrameä»¥ä¾¿æ˜¾ç¤º
        summary_df_numeric = pd.DataFrame(summary_data)

        # åˆ›å»ºæ ¼å¼åŒ–ç‰ˆæœ¬ä»¥ä¾¿æ˜¾ç¤º
        summary_df_display = summary_df_numeric.copy()
        for col in ['Do Nothing', 'Delete All', 'Repair All', 'RL Optimal']:
            summary_df_display[col] = summary_df_display[col].apply(lambda x: f"{x:.3f}")

        # å¹¶æŽ’æ˜¾ç¤ºæ€§èƒ½æ‘˜è¦å’Œå¯è§†åŒ–
        col1, col2 = st.columns([1, 2])

        with col1:
            st.markdown("#### Strategy Performance")
            st.dataframe(summary_df_display.drop('Metric', axis=1), use_container_width=True)

            # åŠ¨ä½œåˆ†å¸ƒæ‘˜è¦ - ç´§å‡‘æ ¼å¼
            st.markdown("#### Action Distribution")

            action_summary = []
            for model_name, results in model_results.items():
                action_counts = results['action_counts']
                total = sum(action_counts.values())
                action_summary.append({
                    'Model': model_name,
                    'No Action': f"{action_counts['no_action'] / total * 100:.1f}%",
                    'Repair': f"{action_counts['repair'] / total * 100:.1f}%",
                    'Delete': f"{action_counts['delete'] / total * 100:.1f}%",
                })

            action_summary_df = pd.DataFrame(action_summary)
            st.dataframe(action_summary_df, use_container_width=True)

        with col2:
            # åˆ›å»ºæ€§èƒ½å¯è§†åŒ–
            fig, ax = plt.subplots(figsize=(8, 4))

            # è®¡ç®—Yè½´èŒƒå›´ï¼Œå‡å°èŒƒå›´ä»¥å¼ºè°ƒå·®å¼‚
            if task_type == 'classification':
                all_values = summary_df_numeric[
                    ['Do Nothing', 'Delete All', 'Repair All', 'RL Optimal']].values.flatten()
                metric_name = "Accuracy"
            else:
                all_values = summary_df_numeric[
                    ['Do Nothing', 'Delete All', 'Repair All', 'RL Optimal']].values.flatten()
                metric_name = "RÂ² Score"

            min_val = max(0, np.min(all_values) - 0.05)
            max_val = min(1, np.max(all_values) + 0.05)

            # ä¸ºæ¯ä¸ªæ¨¡åž‹ç»˜åˆ¶æŠ˜çº¿å›¾ï¼Œçªå‡ºæœ€ä¼˜ç­–ç•¥
            strategies = ['Do Nothing', 'Delete All', 'Repair All', 'RL Optimal']
            markers = ['o', 's', '^', 'D']
            colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']

            for i, model_name in enumerate(summary_df_numeric['Model']):
                values = summary_df_numeric.loc[i, ['Do Nothing', 'Delete All', 'Repair All', 'RL Optimal']].values
                ax.plot(strategies, values, marker=markers[i % len(markers)], markersize=8,
                        linewidth=2, label=model_name, color=colors[i % len(colors)])

                # æ ‡è®° RL Optimal çš„å€¼
                ax.text(3, values[3], f'{values[3]:.3f}', ha='center', va='bottom',
                        fontweight='bold', fontsize=9, color=colors[i % len(colors)])

            ax.set_xlabel('Strategy')
            ax.set_ylabel(metric_name)
            ax.set_ylim(min_val, max_val)
            ax.grid(True, linestyle='--', alpha=0.7)
            ax.legend(title="Models", loc='upper left', fontsize=9)
            ax.set_title(f'Strategy Performance Comparison ({metric_name})')

            plt.tight_layout()
            st.pyplot(fig)

        # å®¹å¿åº¦é˜ˆå€¼åˆ†æž - ä½¿ç”¨å¯è§†åŒ–ä»£æ›¿æ–‡æœ¬
        st.subheader("Tolerance Threshold Analysis")

        # å®¹å¿åº¦é˜ˆå€¼
        tolerance_data = {
            'overall_tolerance': st.session_state['error_rate'] * 0.8,
            'repair_vs_delete': st.session_state['error_rate'] * 0.6,
            'key_feature_tolerance': st.session_state['error_rate'] * 0.7,
            'non_key_feature_tolerance': st.session_state['error_rate'] * 0.9,
            'missing_tolerance': st.session_state['error_rate'] * 0.75,
            'outlier_tolerance': st.session_state['error_rate'] * 0.65
        }

        # ä½¿ç”¨ä¸‰åˆ—å¸ƒå±€å±•ç¤ºå…³é”®é˜ˆå€¼
        col1, col2, col3 = st.columns(3)

        with col1:
            st.metric("Data Quality Threshold", f"{tolerance_data['overall_tolerance']:.3f}",
                      f"{tolerance_data['overall_tolerance'] * 100:.1f}% of error rate")

        with col2:
            st.metric("Repair vs Delete", f"{tolerance_data['repair_vs_delete']:.3f}",
                      "Switching point")

        with col3:
            # ç¡®å®šå“ªç§ç±»åž‹çš„é”™è¯¯æ›´é‡è¦
            error_type = "Outliers" if tolerance_data['missing_tolerance'] > tolerance_data[
                'outlier_tolerance'] else "Missing Values"
            diff = abs(tolerance_data['missing_tolerance'] - tolerance_data['outlier_tolerance'])
            st.metric("Priority Focus", error_type, f"{diff:.3f} difference")

        # è§†è§‰åŒ–å®¹å¿åº¦æ¯”è¾ƒ
        col1, col2 = st.columns([5, 2])

        with col1:
            # åˆ›å»ºå®¹å¿åº¦è·¯çº¿å›¾
            fig, ax = plt.subplots(figsize=(8, 3))

            # ç»˜åˆ¶å®¹å¿åº¦åŒºåŸŸå’Œç­–ç•¥
            ax.axvspan(0, tolerance_data['overall_tolerance'], alpha=0.2, color='#2ecc71', label='Direct Use')
            ax.axvspan(tolerance_data['overall_tolerance'], tolerance_data['repair_vs_delete'], alpha=0.2,
                       color='#f1c40f', label='Quality Check')
            ax.axvspan(tolerance_data['repair_vs_delete'], 1, alpha=0.2, color='#e74c3c', label='Quality Critical')

            # æ·»åŠ å½“å‰é”™è¯¯çŽ‡æ ‡è®°
            ax.axvline(x=st.session_state['error_rate'], color='black', linestyle='-', linewidth=2,
                       label=f'Current Error Rate: {st.session_state["error_rate"]:.2f}')

            # æ·»åŠ æ–‡æœ¬æ ‡ç­¾
            ax.text(tolerance_data['overall_tolerance'] / 2, 0.5, "Direct Use",
                    ha='center', va='center', fontsize=10, fontweight='bold')
            ax.text((tolerance_data['overall_tolerance'] + tolerance_data['repair_vs_delete']) / 2, 0.5, "Repair",
                    ha='center', va='center', fontsize=10, fontweight='bold')
            ax.text((tolerance_data['repair_vs_delete'] + 1) / 2, 0.5, "Delete",
                    ha='center', va='center', fontsize=10, fontweight='bold')

            # è®¾ç½®å›¾è¡¨å±žæ€§
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
            ax.set_yticks([])
            ax.set_xlabel('Error Rate')
            ax.set_title('Error Rate Tolerance Strategy Map')

            # æ·»åŠ é˜ˆå€¼æ ‡è®°
            for threshold, name in [(tolerance_data['overall_tolerance'], 'Quality\nThreshold'),
                                    (tolerance_data['repair_vs_delete'], 'Repair/Delete\nThreshold')]:
                ax.axvline(x=threshold, color='gray', linestyle='--', alpha=0.7)
                ax.text(threshold, 0.1, f"{threshold:.3f}", ha='center', va='bottom',
                        fontsize=8, bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.3'))
                ax.text(threshold, 0.9, name, ha='center', va='top', fontsize=8,
                        rotation=90, bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.3'))

            plt.tight_layout()
            st.pyplot(fig)

        with col2:
            #é›·è¾¾å›¾
            categories = ['Overall', 'Repair/Delete', 'Key Features',
                          'Non-key Features', 'Missing Values', 'Outliers']
            values = [tolerance_data['overall_tolerance'],
                      tolerance_data['repair_vs_delete'],
                      tolerance_data['key_feature_tolerance'],
                      tolerance_data['non_key_feature_tolerance'],
                      tolerance_data['missing_tolerance'],
                      tolerance_data['outlier_tolerance']]

            # é—­åˆå¤šè¾¹å½¢
            values_for_radar = values + [values[0]]
            angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
            angles += angles[:1]

            # è®¡ç®—æœ€å¤§å€¼ç”¨äºŽè®¾ç½®æ›´åˆé€‚çš„Yè½´ä¸Šé™ï¼ˆè®©å›¾æ›´â€œæ’‘å¼€â€ï¼‰
            max_value = max(values)
            ymax = min(1.0, max_value + 0.2)  # è‡ªåŠ¨å‘ä¸Šæ‰©å±•20%ï¼Œæœ€å¤šä¸è¶…è¿‡1

            fig, ax = plt.subplots(figsize=(4, 4), subplot_kw=dict(polar=True))

            # ç»˜åˆ¶åŒºåŸŸ
            ax.fill(angles, values_for_radar, color='#3498db', alpha=0.3)
            ax.plot(angles, values_for_radar, color='#3498db', linewidth=2)

            # æ•°æ®æ ‡ç­¾
            for i, (angle, value) in enumerate(zip(angles[:-1], values)):
                ax.text(angle, value + ymax * 0.05, f"{value:.2f}",
                        ha='center', va='center', fontsize=9,
                        bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.2'))

            # è®¾ç½®åˆ»åº¦å’Œç½‘æ ¼
            ax.set_xticks(angles[:-1])
            ax.set_xticklabels(categories, fontsize=10)
            ax.set_ylim(0, ymax)
            ax.set_yticks(np.linspace(0, ymax, 5))  # æ·»åŠ ç½‘æ ¼çº¿
            ax.set_yticklabels([f'{v:.2f}' for v in np.linspace(0, ymax, 5)], fontsize=8, color='gray')
            ax.yaxis.grid(True, linestyle='dashed', alpha=0.4)
            ax.xaxis.grid(True, linestyle='dotted', alpha=0.4)

            # æ ‡é¢˜
            ax.set_title('Tolerance Thresholds by Category', pad=20, fontsize=12)

            plt.tight_layout()
            st.pyplot(fig)

        # è¯¦ç»†çš„é˜ˆå€¼è¯´æ˜Ž - æ”¾å…¥å¯æŠ˜å åŒºåŸŸ
        with st.expander("View Detailed Tolerance Analysis", expanded=False):
            st.markdown("### Tolerance Thresholds Report")

            st.markdown("#### 1. Overall Tolerance Threshold")
            st.markdown(f"- **Overall data quality tolerance threshold:** {tolerance_data['overall_tolerance']:.3f}")
            st.markdown(
                f"- When error rate exceeds {tolerance_data['overall_tolerance'] * 100:.1f}%, data preprocessing is required.")

            st.markdown("#### 2. Repair vs. Delete Strategy Threshold")
            st.markdown(f"- **Repair/Delete threshold:** {tolerance_data['repair_vs_delete']:.3f}")
            st.markdown(
                f"- When error rate is below {tolerance_data['repair_vs_delete'] * 100:.1f}%, repair strategy is preferred.")
            st.markdown(
                f"- When error rate is above {tolerance_data['repair_vs_delete'] * 100:.1f}%, delete strategy is preferred.")

            st.markdown("#### 3. Feature Importance Analysis")
            st.markdown(f"- **Key feature tolerance threshold:** {tolerance_data['key_feature_tolerance']:.3f}")
            st.markdown(f"- **Non-key feature tolerance threshold:** {tolerance_data['non_key_feature_tolerance']:.3f}")
            diff = tolerance_data['non_key_feature_tolerance'] - tolerance_data['key_feature_tolerance']
            st.markdown(f"- **Conclusion:** Key features have lower tolerance (difference: {diff:.3f})")
            st.markdown("  - Models require higher data quality for important features.")

            st.markdown("#### 4. Error Type Analysis")
            st.markdown(f"- **Missing value tolerance threshold:** {tolerance_data['missing_tolerance']:.3f}")
            st.markdown(f"- **Outlier tolerance threshold:** {tolerance_data['outlier_tolerance']:.3f}")
            diff = tolerance_data['missing_tolerance'] - tolerance_data['outlier_tolerance']
            if diff > 0:
                st.markdown(f"- **Conclusion:** Models have lower tolerance for outliers (difference: {abs(diff):.3f})")
                st.markdown("  - Priority should be given to handling outliers.")
            else:
                st.markdown(
                    f"- **Conclusion:** Models have lower tolerance for missing values (difference: {abs(diff):.3f})")
                st.markdown("  - Priority should be given to handling missing values.")


        # ç¡®å®šæœ€ä½³æ¨¡åž‹
        def determine_best_model(model_results, task_type):
            """
            ç¡®å®šæœ€ä½³æ¨¡åž‹ï¼ŒåŸºäºŽå¤šä¸ªè¯„ä¼°å› ç´ 

            å‚æ•°:
            - model_results: åŒ…å«å„æ¨¡åž‹æ€§èƒ½ç»“æžœçš„å­—å…¸
            - task_type: 'classification' æˆ– 'regression'

            è¿”å›ž:
            - best_model: æœ€ä½³æ¨¡åž‹åç§°
            - model_scores: å„æ¨¡åž‹çš„è¯¦ç»†è¯„åˆ†
            """
            # åˆå§‹åŒ–è¯„åˆ†è®°å½•
            model_scores = {}

            for model_name, results in model_results.items():
                strat_perf = results['strategy_performance']
                action_counts = results['action_counts']
                total_actions = sum(action_counts.values())

                # 1. åŸºæœ¬æ€§èƒ½åˆ†æ•° (0-100åˆ†)
                if task_type == 'classification':
                    base_score = strat_perf['RL_optimal']["accuracy"] * 100
                    baseline_score = strat_perf['do_nothing']["accuracy"] * 100
                else:
                    base_score = strat_perf['RL_optimal']["r2_score"] * 100
                    baseline_score = strat_perf['do_nothing']["r2_score"] * 100

                # 2. æ”¹è¿›å¹…åº¦ (0-50åˆ†)
                # è®¡ç®—ç›¸å¯¹äºŽä¸åšä»»ä½•æ“ä½œçš„æ”¹è¿›ç™¾åˆ†æ¯”
                improvement = base_score - baseline_score
                improvement_score = min(50, max(0, improvement * 5))  # æ¯æé«˜1ä¸ªç™¾åˆ†ç‚¹å¾—5åˆ†ï¼Œæœ€é«˜50åˆ†

                # 3. é²æ£’æ€§è¯„åˆ† (0-30åˆ†)
                # è®¡ç®—ä¸åŒç­–ç•¥ä¸‹çš„æ€§èƒ½ç¨³å®šæ€§
                if task_type == 'classification':
                    performances = [
                        strat_perf['do_nothing']["accuracy"],
                        strat_perf['delete_all']["accuracy"],
                        strat_perf['repair_all']["accuracy"],
                        strat_perf['RL_optimal']["accuracy"]
                    ]
                else:
                    performances = [
                        strat_perf['do_nothing']["r2_score"],
                        strat_perf['delete_all']["r2_score"],
                        strat_perf['repair_all']["r2_score"],
                        strat_perf['RL_optimal']["r2_score"]
                    ]

                # è®¡ç®—æ€§èƒ½çš„æ ‡å‡†å·® (è¶Šä½Žè¶Šç¨³å®š)
                stability = np.std(performances)
                stability_score = 30 * (1 - min(stability * 5, 1))  # æ ‡å‡†å·®è¶Šä½Žï¼Œå¾—åˆ†è¶Šé«˜

                # 4. ç­–ç•¥æ•ˆçŽ‡ (0-20åˆ†)
                # è¯„ä¼°æ¨¡åž‹çš„åŠ¨ä½œé€‰æ‹©æ˜¯å¦ç»æµŽé«˜æ•ˆ
                # å‡è®¾ä¿®å¤æ¯”åˆ é™¤æ›´é«˜æ•ˆï¼Œæ— æ“ä½œè¡¨ç¤ºä¸éœ€è¦å¤„ç†
                repair_rate = action_counts['repair'] / total_actions if total_actions > 0 else 0
                delete_rate = action_counts['delete'] / total_actions if total_actions > 0 else 0
                no_action_rate = action_counts['no_action'] / total_actions if total_actions > 0 else 0

                # é¦–é€‰ä¿®å¤è€Œéžåˆ é™¤ï¼Œä½†ä¹Ÿè¦ä¿æŒä¸€å®šå¹³è¡¡
                strategy_score = 20 * (0.5 * repair_rate + 0.3 * no_action_rate + 0.2 * (1 - delete_rate))

                # 5. ç»¼åˆè¯„åˆ† (0-200åˆ†)
                total_score = base_score + improvement_score + stability_score + strategy_score

                # è®°å½•è¯¦ç»†è¯„åˆ†
                model_scores[model_name] = {
                    'base_score': base_score,
                    'improvement_score': improvement_score,
                    'stability_score': stability_score,
                    'strategy_score': strategy_score,
                    'total_score': total_score
                }

            # é€‰æ‹©ç»¼åˆè¯„åˆ†æœ€é«˜çš„æ¨¡åž‹
            best_model = max(model_scores.items(), key=lambda x: x[1]['total_score'])[0]

            return best_model, model_scores


        # ç¡®å®šæœ€ä½³æ¨¡åž‹å¹¶æ˜¾ç¤ºè¯„åˆ†ç»“æžœ
        best_model, model_scores = determine_best_model(model_results, task_type)

        # æœ€ç»ˆå»ºè®®éƒ¨åˆ† - ç¾Žè§‚å¡ç‰‡å¼å¸ƒå±€
        st.subheader("Final Recommendations")

        col1, col2 = st.columns(2)

        # å·¦ä¾§æ˜¾ç¤ºæœ€ä½³æ¨¡åž‹å’Œç­–ç•¥
        with col1:
            st.markdown(f"""
            <div style="background-color:#f0f2f6;padding:15px;border-radius:10px;margin-bottom:10px;">
                <h3 style="margin:0;color:#1f77b4;">Best Model</h3>
                <h2 style="margin:5px 0;color:#d62728;">{best_model.upper()}</h2>
                <p style="margin:0;font-size:0.9em;">Score: {model_scores[best_model]['total_score']:.1f}/200</p>
            </div>

            <div style="background-color:#f0f2f6;padding:15px;border-radius:10px;">
                <h3 style="margin:0;color:#1f77b4;">Best Strategy</h3>
                <h2 style="margin:5px 0;color:#d62728;">RL Optimal</h2>
                <p style="margin:0;font-size:0.9em;">Performance: {model_scores[best_model]['base_score']:.1f}%</p>
            </div>
            """, unsafe_allow_html=True)

        # å³ä¾§æ˜¾ç¤ºé‡è¦é˜ˆå€¼
        with col2:
            st.markdown(f"""
            <div style="background-color:#f0f2f6;padding:15px;border-radius:10px;margin-bottom:10px;">
                <h3 style="margin:0;color:#1f77b4;">Tolerance Threshold</h3>
                <h2 style="margin:5px 0;color:#d62728;">{tolerance_data['overall_tolerance']:.3f}</h2>
                <p style="margin:0;font-size:0.9em;">Critical point for data quality</p>
            </div>

            <div style="background-color:#f0f2f6;padding:15px;border-radius:10px;">
                <h3 style="margin:0;color:#1f77b4;">Error Type Focus</h3>
                <h2 style="margin:5px 0;color:#d62728;">
                {'Outliers' if tolerance_data['missing_tolerance'] > tolerance_data['outlier_tolerance'] else 'Missing Values'}
                </h2>
                <p style="margin:0;font-size:0.9em;">Priority for data cleaning</p>
            </div>
            """, unsafe_allow_html=True)

        # æ¨¡åž‹è¯„åˆ†å¯è§†åŒ–
        st.subheader("Model Score Breakdown")

        # åˆ›å»ºè¯„åˆ†è¡¨æ ¼æ•°æ®
        score_data = []
        for model_name, scores in model_scores.items():
            score_data.append({
                'Model': model_name,
                'Base Performance': f"{scores['base_score']:.1f}",
                'Improvement': f"{scores['improvement_score']:.1f}",
                'Stability': f"{scores['stability_score']:.1f}",
                'Strategy': f"{scores['strategy_score']:.1f}",
                'Total Score': f"{scores['total_score']:.1f}"
            })

        score_df = pd.DataFrame(score_data)

        col1, col2 = st.columns([1, 2])

        with col1:
            st.dataframe(score_df, use_container_width=True)

            with st.expander("Score Calculation Methodology", expanded=False):
                st.markdown("""
                **Model scoring is based on:**

                1. **Base Performance (0-100)**: Raw performance of RL optimal strategy
                2. **Improvement (0-50)**: Improvement over "do nothing" baseline
                3. **Stability (0-30)**: Consistency across different strategies
                4. **Strategy (0-20)**: Preference for efficient repair over deletion

                Total score is the sum of all components (max 200).
                """)

        with col2:
            # åˆ›å»ºå †å æ¡å½¢å›¾å±•ç¤ºè¯„åˆ†ç»†åˆ†
            fig, ax = plt.subplots(figsize=(8, 4))

            models = list(model_scores.keys())
            x = np.arange(len(models))
            width = 0.6

            # å‡†å¤‡æ•°æ®
            base_scores = [model_scores[m]['base_score'] for m in models]
            improvement_scores = [model_scores[m]['improvement_score'] for m in models]
            stability_scores = [model_scores[m]['stability_score'] for m in models]
            strategy_scores = [model_scores[m]['strategy_score'] for m in models]

            # åˆ›å»ºå †å æ¡å½¢å›¾
            p1 = ax.bar(x, base_scores, width, label='Base Performance', color='#3498db')
            p2 = ax.bar(x, improvement_scores, width, bottom=base_scores, label='Improvement', color='#2ecc71')

            bottom = np.array(base_scores) + np.array(improvement_scores)
            p3 = ax.bar(x, stability_scores, width, bottom=bottom, label='Stability', color='#f1c40f')

            bottom = bottom + np.array(stability_scores)
            p4 = ax.bar(x, strategy_scores, width, bottom=bottom, label='Cost', color='#e74c3c')

            # æ·»åŠ æ€»åˆ†æ ‡ç­¾
            for i, model in enumerate(models):
                total = model_scores[model]['total_score']
                ax.text(i, total + 5, f'{total:.1f}', ha='center', va='bottom', fontweight='bold')

                # å¦‚æžœæ˜¯æœ€ä½³æ¨¡åž‹ï¼Œæ·»åŠ æ ‡è®°
                if model == best_model:
                    ax.text(i, total + 15, 'â˜… BEST', ha='center', va='bottom',
                            color='#e74c3c', fontweight='bold', fontsize=12)

            # è®¾ç½®å›¾è¡¨å±žæ€§
            ax.set_ylabel('Score')
            ax.set_title('Model Performance Score Breakdown')
            ax.set_xticks(x)
            ax.set_xticklabels(models)
            ax.legend(loc='upper right', fontsize=8)
            ax.set_ylim(0, max([model_scores[m]['total_score'] for m in models]) * 1.2)

            plt.tight_layout()
            st.pyplot(fig)

        # å¯¼å‡ºé€‰é¡¹éƒ¨åˆ† - ä½¿ç”¨å¡ç‰‡å¼å¸ƒå±€
        st.subheader("Export Options")

        col1, col2, col3 = st.columns(3)

        with col1:
            st.markdown("""
            <div style="background-color:#f0f2f6;padding:15px;border-radius:10px;text-align:center;">
                <h4 style="margin:0;color:#666;">Data Export</h4>
                <p style="margin:10px 0;">Export processed data and results</p>
            </div>
            """, unsafe_allow_html=True)

            export_format = st.radio("Select export format:", ["CSV", "Excel", "JSON"])

            # æ£€æŸ¥æ˜¯å¦æœ‰å·²ç”Ÿæˆçš„å›¾åƒæ–‡ä»¶
            task_type = st.session_state.get('task_type', 'classification')
            error_rate = st.session_state['error_rate']
            all_images = find_images(task_type, error_rate)

            if st.button("Export Data & Results"):
                with st.spinner("Preparing export..."):
                    # èŽ·å–RLå¤„ç†åŽçš„æ¸…æ´æ•°æ®
                    env = st.session_state['env']
                    processed_df = env.df.copy()

                    # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å
                    timestamp = time.strftime("%Y%m%d-%H%M%S")

                    if export_format == "CSV":
                        # å¯¼å‡ºä¸ºCSV
                        csv = processed_df.to_csv(index=False)
                        st.download_button(
                            label="Download processed data (CSV)",
                            data=csv,
                            file_name=f"cbtad_processed_data_{timestamp}.csv",
                            mime="text/csv",
                        )

                        # å‡†å¤‡ç»“æžœæ‘˜è¦
                        results_df = pd.DataFrame(summary_data)
                        csv_results = results_df.to_csv(index=False)
                        st.download_button(
                            label="Download results summary (CSV)",
                            data=csv_results,
                            file_name=f"cbtad_results_{timestamp}.csv",
                            mime="text/csv",
                        )
                    elif export_format == "Excel":
                        # å¯¹äºŽExcelå¯¼å‡ºï¼Œæˆ‘ä»¬éœ€è¦å…ˆä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
                        excel_file = f"cbtad_export_{timestamp}.xlsx"
                        with pd.ExcelWriter(excel_file) as writer:
                            processed_df.to_excel(writer, sheet_name="Processed Data", index=False)
                            pd.DataFrame(summary_data).to_excel(writer, sheet_name="Performance Summary", index=False)
                            pd.DataFrame(action_summary).to_excel(writer, sheet_name="Action Summary", index=False)

                            # æ·»åŠ æ¨¡åž‹å¾—åˆ†æ•°æ®
                            score_df = pd.DataFrame(score_data)
                            score_df.to_excel(writer, sheet_name="Model Scores", index=False)

                            # æ·»åŠ å®¹å¿åº¦é˜ˆå€¼æ•°æ®
                            tolerance_df = pd.DataFrame([tolerance_data])
                            tolerance_df.to_excel(writer, sheet_name="Tolerance Thresholds", index=False)

                        # è¯»å–æ–‡ä»¶å¹¶æä¾›ä¸‹è½½æŒ‰é’®
                        with open(excel_file, "rb") as f:
                            excel_data = f.read()

                        st.download_button(
                            label="Download Complete Report (Excel)",
                            data=excel_data,
                            file_name=f"cbtad_results_{timestamp}.xlsx",
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                        )

                        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                        os.remove(excel_file)

                    else:  # JSON
                        # å‡†å¤‡JSONå¯¼å‡º
                        export_data = {
                            "metadata": {
                                "timestamp": timestamp,
                                "task_type": task_type,
                                "error_rate": st.session_state['error_rate'],
                                "error_composition": {
                                    "missing": st.session_state['missing_ratio'],
                                    "outliers": st.session_state['outlier_ratio'],
                                    "noise": st.session_state['noise_ratio']
                                }
                            },
                            "results": {
                                "performance_summary": summary_data,
                                "action_summary": action_summary,
                                "model_scores": {model: scores for model, scores in model_scores.items()},
                                "tolerance_thresholds": tolerance_data,
                                "best_model": best_model
                            }
                        }

                        # è½¬æ¢ä¸ºJSON
                        json_str = json.dumps(export_data, indent=2)

                        st.download_button(
                            label="Download Results (JSON)",
                            data=json_str,
                            file_name=f"cbtad_results_{timestamp}.json",
                            mime="application/json",
                        )

                    st.success("Export prepared successfully!")

        with col2:
            st.markdown("""
            <div style="background-color:#f0f2f6;padding:15px;border-radius:10px;text-align:center;">
                <h4 style="margin:0;color:#666;">Visualizations Export</h4>
                <p style="margin:10px 0;">Export generated charts and visualizations</p>
            </div>
            """, unsafe_allow_html=True)

            # æŸ¥æ‰¾å½“å‰æ‰€æœ‰å¯ç”¨çš„å›¾åƒ
            all_images = find_images(task_type, error_rate)

            if all_images:
                st.write(f"Found {len(all_images)} visualization images.")

                if st.checkbox("Preview visualizations", value=False):
                    # åˆ›å»ºä¸€ä¸ªå›¾ç‰‡é¢„è§ˆç½‘æ ¼
                    preview_cols = st.columns(1)
                    for i, img_path in enumerate(all_images[:4]):  # é™åˆ¶é¢„è§ˆæ•°é‡
                        with preview_cols[0]:
                            img = Image.open(img_path)
                            caption = os.path.basename(img_path)
                            st.image(img, caption=caption, use_container_width=True)

                    if len(all_images) > 4:
                        st.info(f"+ {len(all_images) - 4} more visualizations available for download")
                download_cols = st.columns(2)
                with download_cols[0]:
                    if st.button("Export All Visualizations"):
                        # åˆ›å»ºä¸€ä¸ªä¸´æ—¶ZIPæ–‡ä»¶
                        import zipfile

                        timestamp = time.strftime("%Y%m%d-%H%M%S")
                        zip_file = f"cbtad_visualizations_{timestamp}.zip"
                        with zipfile.ZipFile(zip_file, 'w') as zipf:
                            for img in all_images:
                                zipf.write(img, arcname=os.path.basename(img))

                        # è¯»å–ZIPæ–‡ä»¶å¹¶æä¾›ä¸‹è½½
                        with open(zip_file, "rb") as f:
                            zip_data = f.read()
                        st.download_button(
                            label="Download All Visualizations (ZIP)",
                            data=zip_data,
                            file_name=f"cbtad_visualizations_{timestamp}.zip",
                            mime="application/zip",
                        )

                        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                        os.remove(zip_file)
                        st.success("Visualizations packaged successfully!")
                with download_cols[1]:
                    # æ·»åŠ æŠ¥å‘Šç”Ÿæˆé€‰é¡¹
                    if st.button("Generate PDF Report"):
                        with st.spinner("Generating visualization report..."):
                            # ä½¿ç”¨matplotlibçš„PDFåŽç«¯åˆ›å»ºæŠ¥å‘Š
                            try:
                                from matplotlib.backends.backend_pdf import PdfPages

                                timestamp = time.strftime("%Y%m%d-%H%M%S")
                                pdf_file = f"cbtad_report_{timestamp}.pdf"

                                with PdfPages(pdf_file) as pdf:
                                    # åˆ›å»ºæ ‡é¢˜é¡µ
                                    plt.figure(figsize=(8.5, 11))
                                    plt.text(0.5, 0.6, "DemandClean Visualization Report",
                                             ha='center', fontsize=24, weight='bold')
                                    plt.text(0.5, 0.5, f"Task Type: {task_type.upper()}",
                                             ha='center', fontsize=16)
                                    plt.text(0.5, 0.45, f"Error Rate: {error_rate:.2f}",
                                             ha='center', fontsize=16)
                                    plt.text(0.5, 0.4, f"Generated: {timestamp}",
                                             ha='center', fontsize=16)
                                    plt.text(0.5, 0.3, f"Best Model: {best_model.upper()}",
                                             ha='center', fontsize=20, color='#d62728')
                                    plt.axis('off')
                                    pdf.savefig()
                                    plt.close()

                                    # æ·»åŠ æ‰€æœ‰æ‰¾åˆ°çš„å›¾åƒ
                                    for img_path in all_images:
                                        img = Image.open(img_path)

                                        # å°†PILå›¾åƒè½¬æ¢ä¸ºmatplotlibå›¾åƒ
                                        plt.figure(figsize=(8.5, 11))
                                        plt.imshow(img)
                                        plt.title(os.path.basename(img_path), fontsize=14)
                                        plt.axis('off')
                                        pdf.savefig()
                                        plt.close()

                                # æä¾›PDFä¸‹è½½
                                with open(pdf_file, "rb") as f:
                                    pdf_data = f.read()

                                st.download_button(
                                    label="Download PDF Report",
                                    data=pdf_data,
                                    file_name=pdf_file,
                                    mime="application/pdf",
                                )

                                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                                os.remove(pdf_file)
                                st.success("Report generated successfully!")

                            except ImportError:
                                st.warning("PDF generation requires additional libraries.")
                                st.info("Please install matplotlib with PDF backend support.")
            else:
                st.info("No visualization images found. Generate visualizations in previous steps first.")

        with col3:
            st.markdown("""
            <div style="background-color:#f0f2f6;padding:15px;border-radius:10px;text-align:center;">
                <h4 style="margin:0;color:#666;">Model Deployment</h4>
                <p style="margin:10px 0;">Download trained models for deployment</p>
            </div>
            """, unsafe_allow_html=True)

            # èŽ·å–å½“å‰æ¨¡åž‹æ–‡ä»¶çš„åˆ—è¡¨
            model_files = glob.glob(f"dqn_agent_{task_type}_*.h5")

            if model_files:
                st.write(f"Found {len(model_files)} trained models:")

                # åˆ›å»ºæ¨¡åž‹åˆ—è¡¨å’Œé€‰æ‹©æ¡†
                model_options = [os.path.basename(file) for file in model_files]
                selected_model = st.selectbox("Select model to download:", model_options)

                # æ˜¾ç¤ºæ‰€é€‰æ¨¡åž‹çš„ä¿¡æ¯
                # ä¿®å¤æ¨¡åž‹ä¿¡æ¯è§£æžä»£ç 
                if selected_model:
                    model_path = os.path.join("", selected_model)

                    # ä½¿ç”¨æ›´å¥å£®çš„æ–¹å¼è§£æžæ¨¡åž‹æ–‡ä»¶å
                    import re

                    # ä»Žæ–‡ä»¶åä¸­æå–ä»»åŠ¡ç±»åž‹å’Œæ¨¡åž‹ç±»åž‹
                    model_name = selected_model.replace("dqn_agent_", "").replace(".h5", "")

                    # å°è¯•ä»Žæ–‡ä»¶åä¸­æå–é”™è¯¯çŽ‡ (æŸ¥æ‰¾ç±»ä¼¼ 0.30 çš„æµ®ç‚¹æ•°)
                    error_rate_match = re.search(r'(\d+\.\d+)', model_name)
                    model_error = float(error_rate_match.group(1)) if error_rate_match else "Multi-Rate"

                    # æå–ä»»åŠ¡ç±»åž‹ (classification æˆ– regression)
                    task_type = "classification" if "classification" in model_name else "regression"

                    # æå–æ¨¡åž‹ç±»åž‹ (ç§»é™¤ä»»åŠ¡ç±»åž‹å’Œé”™è¯¯çŽ‡åŽå‰©ä½™çš„éƒ¨åˆ†)
                    model_type = model_name.replace(f"{task_type}_", "")
                    if error_rate_match:
                        model_type = model_type.replace(error_rate_match.group(1), "").strip("_")

                    # æ˜¾ç¤ºæ¨¡åž‹å¡ç‰‡
                    st.markdown(f"""
                    <div style="background-color:#f8f9fa;padding:10px;border-radius:5px;margin-bottom:10px;">
                        <h5 style="margin:0;color:#333;">{selected_model}</h5>
                        <p style="margin:5px 0;font-size:0.9em;">
                            <span style="color:#666;">Task:</span> <b>{task_type}</b><br>
                            <span style="color:#666;">Model:</span> <b>{model_type}</b><br>
                            <span style="color:#666;">Error Rate:</span> <b>{model_error if isinstance(model_error, str) else f"{model_error:.2f}"}</b>
                        </p>
                    </div>
                    """, unsafe_allow_html=True)

                    # ä½ çš„æœ¬åœ°æ–‡ä»¶è·¯å¾„
                    file_path = "./"+selected_model

                    # æ‰“å¼€æ–‡ä»¶å¹¶åˆ›å»ºä¸‹è½½æŒ‰é’®
                    with open(file_path, "rb") as f:
                        file_bytes = f.read()
                        st.download_button(
                            label="ðŸ“¥ ä¸‹è½½æ¨¡åž‹æ–‡ä»¶",
                            data=file_bytes,
                            file_name=model_name,  # ä¸‹è½½æ—¶ä¿å­˜çš„æ–‡ä»¶å
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                        )

            else:
                st.info("No trained models found. Complete the training step to generate models.")
